<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: tools | Confessions of a Wall Street Programmer]]></title>
  <link href="http://btorpey.github.io/blog/categories/tools/atom.xml" rel="self"/>
  <link href="http://btorpey.github.io/"/>
  <updated>2017-05-23T21:35:58-04:00</updated>
  <id>http://btorpey.github.io/</id>
  <author>
    <name><![CDATA[Bill Torpey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[We Don't Need No Stinkin' Databases]]></title>
    <link href="http://btorpey.github.io/blog/2017/05/10/join/"/>
    <updated>2017-05-10T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2017/05/10/join</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/Gold_Hat_portrayed_by_Alfonso_Bedoya.jpg" width="220" height="162">

I've been working on performance analysis recently, and a large part of that is scraping log files to capture interesting events and chart them.

I'm continually surprised by the things that you can do using plain old bash and his friends, but this latest one took the cake for me.

<!-- more -->

Did you know that Linux includes a utility named `join`?  Me neither.  Can you guess what it does?  Yup, that's right -- it does the equivalent of a database join across plain text files.

Let me clarify that with a real-world example -- one of the datasets I've been analyzing counts the number of messages sent and received in a format roughly like this:

|Timestamp| Recv |
|:--------|-----:|
| HH:MM:SS| x |

<br>
Complicating matters is that sent and received messages are parsed out separately, so we also have a separate file that looks like this:

|Timestamp| Send |
|:--------|-----:|
| HH:MM:SS| y |

<br>
But what we really want is something like this:

|Timestamp| Recv | Send |
|:--------|-----:|------:|
| HH:MM:SS| x | y |

<br>
Here are snips from the two files:

    $ cat recv.txt
    Timestamp	Recv
    2016/10/25-16:04:58	7
    2016/10/25-16:04:59	1
    2016/10/25-16:05:00	7
    2016/10/25-16:05:01	9
    2016/10/25-16:05:28	3
    2016/10/25-16:05:31	9
    2016/10/25-16:05:58	3
    2016/10/25-16:06:01	9
    2016/10/25-16:06:28	3
    $ cat send.txt
    Timestamp	Send
    2016/10/25-16:04:58	6
    2016/10/25-16:05:01	18
    2016/10/25-16:05:28	3
    2016/10/25-16:05:31	9
    2016/10/25-16:05:58	3
    2016/10/25-16:06:01	9
    2016/10/25-16:06:28	3
    2016/10/25-16:06:31	9
    2016/10/25-16:06:58	3


I had stumbled across the `join` command and thought it would be a good way to combine the two files.

Doing a simple join with no parameters gives this:

    $ join recv.txt send.txt
    Timestamp Recv Send
    2016/10/25-16:04:58 7 6
    2016/10/25-16:05:01 9 18
    2016/10/25-16:05:28 3 3
    2016/10/25-16:05:31 9 9
    2016/10/25-16:05:58 3 3
    2016/10/25-16:06:01 9 9
    2016/10/25-16:06:28 3 3

As you can see, we're missing some of the measurements.  This is because by default `join` does an [inner join](https://en.wikipedia.org/wiki/Join_(SQL)#Inner_join) of the two files (the intersection, in set theory).

That's OK, but not really what we want.  We really need to be able to reflect each value from both datasets, and for that we need an [outer join](https://en.wikipedia.org/wiki/Join_(SQL)#Outer_join), or union.

It turns out that `join` can do that too, although the syntax is a bit more complicated:

    $ join -t $'\t' -o 0,1.2,2.2 -a 1 -a 2 recv.txt send.txt
    Timestamp	Recv	Send
    2016/10/25-16:04:58	7	6
    2016/10/25-16:04:59	1
    2016/10/25-16:05:00	7
    2016/10/25-16:05:01	9	18
    2016/10/25-16:05:28	3	3
    2016/10/25-16:05:31	9	9
    2016/10/25-16:05:58	3	3
    2016/10/25-16:06:01	9	9
    2016/10/25-16:06:28	3	3
    2016/10/25-16:06:31		9
    2016/10/25-16:06:58		3


A brief run-down of the parameters is probably in order:

|Parameter | Description
|----------|------------
| `-t $'\t'` | The `-t` parameter tells `join` what to use as the separator between fields.  The tab character is the best choice, as most Unix utilities assume that by default, and both Excel and Numbers can work with tab-delimited files.<br>The leading dollar-sign is a [trick](https://unix.stackexchange.com/a/46931/198530) used to to pass a literal tab character on the command line  .
| `-o 0,1.2,2.2` | Specifies which fields to output.  In this case, we want the "join field" (in this case, the first field from both files), then the second field from file #1, then the second field from file #2.
| `-a 1` | Tells `join` that we want **all** the fields from file #1.
| `-a 2` | Ditto for file #2.

<br>
As you can probably see, you can also get fancy and do things like left outer joins and right outer joins, depending on the parameters passed.

Of course, you could easily import these text files into a "real" database and generate reports that way.  But, you can accomplish a surprising amount of data manipulation and reporting with Linux's built-in utilities and plain old text files.

### Acknowledgements
I couldn't remember where I had originally seen the `join` command, but recently found it again in a [nice post by Alexander Blagoev](http://ablagoev.github.io/linux/adventures/commands/2017/02/19/adventures-in-usr-bin.html).  Check it out for even more obscure commands!  And, thanks Alexander!  

And thanks also to Igor for his own [very nice post](http://shiroyasha.io/coreutils-that-you-might-not-know.html) that led  me back to Alexander's.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Even Mo' Static]]></title>
    <link href="http://btorpey.github.io/blog/2016/11/12/even-mo-static/"/>
    <updated>2016-11-12T00:00:00-05:00</updated>
    <id>http://btorpey.github.io/blog/2016/11/12/even-mo-static</id>
    <content type="html"><![CDATA[<p><img class="left" src="/images/vandergraaf.jpg" width="139" height="122">

- Will be replaced with the ToC, excluding the "Contents" header
{:toc}

A while back I wrote [an article](/blog/2016/04/07/mo-static/) that compared cppcheck and clang's static analyzers (clang-check and clang-tidy).  The folks who make [PVS-Studio](http://www.viva64.com/en/pvs-studio/) (the guys with the unicorn mascot that you've probably been seeing a lot of lately) saw the article, and suggested that I take a look at their Linux port, which was then in beta test, and write about it.

So I did.  Read on for an overview of PVS-Studio, and how it compared to [cppcheck](http://cppcheck.sourceforge.net/).

<!-- more -->

In [the earlier article](/blog/2016/04/07/mo-static/), I used a [benchmark suite](https://github.com/regehr/itc-benchmarks) developed by Toyota ITC, and written about by [John Regehr](http://blog.regehr.org/archives/1217), who is a professor of Computer Science at the University of Utah.  The ITC suite consists of code that is specially written to exhibit certain errors that can be detected by static analysis, so that the code can be used to evaluate the performance of different tools.

In this article, I am going to use the same test suite to evaluate  PVS-Studio, and to compare it against cppcheck.  I'll also talk about my experience using both tools to analyze two relatively large real-world codebases that I help maintain as part of my day job.

## TL;DR
Using any static analysis tool is better than using none, and in general the more the merrier.  Each tool has its own design philosophy, and corresponding strengths and weaknesses.

Daniel Marjam√§ki[^daniel] and the maintainers of [cppcheck](http://cppcheck.sourceforge.net/) have done a terrific job creating a free tool that can go head-to-head with expensive commercial offerings.  You can't go wrong with cppcheck, either as a gentle introduction to static analysis, or as the one-and-only tool for the budget-conscious.  But don't take my word for it -- the Debian project uses cppcheck as part of its [Debian Automated Code Analysis](https://qa.debian.org/daca/) project to check over 100GB of C++ source code.

[^daniel]: Daniel was recently interviewed on [CppCast](http://cppcast.com/2016/11/daniel-marjamaki/). 

[PVS-Studio](http://www.viva64.com/en/pvs-studio/) is also a terrific tool, but it is definitely _not_ free.  (When a product [doesn't have published prices](http://www.viva64.com/en/order/), you know it's going to cost serious money).

Whether PVS-Studio is worth the price is a judgement call, but if it can find just one bug that would have triggered a crash in production it will have paid for itself many times over. 

And while PVS-Studio doesn't appear to have been adopted by a high-profile project like Debian, the folks who make it are certainly not shy about running various open-source projects through their tool and [reporting the results](http://www.viva64.com/en/inspections/).  

So, if your budget can handle it, use both.  If money is a concern, then you may want to start out with cppcheck and use that to help build a case for spending the additional coin that it will take to include commercial tools like PVS-Studio in your toolbox.

Note also that PVS-Studio offers a trial version[^free], so you can give it a go on your own code, which is, after all, the best way to see what the tool can do.  And, if you use the provided [helper scripts](/pages/REAME.md/index.html) ([repo here](https://github.com/btorpey/static)), your results will be in a format that makes it easy to compare the tools.

[^free]: The folks at PVS-Studio asked me to mention that they've also recently introduced a free version of their software for educational purposes. The free version does have some strings attached, see [this post](http://www.viva64.com/en/b/0457/) for details.

## Methodology
In comparing cppcheck and PVS-Studio, I used the ITC test suite that I wrote about in an [earlier article](/blog/2016/04/07/mo-static/).  I also used both tools to analyze real-world code bases which I deal with on a day-to-day basis and that I am intimately familiar with.

### ITC test suite
The ITC test suite that I've been using to compare static analyzers is intended to provide a common set of source files that can be used as input to various static analysis tools.  It includes both real errors, as well as "false positives" intended to trick the tools into flagging legitimate code as an error.

So far, so good, and it's certainly very helpful to know where the errors are (and are not) when evaluating a static analysis tool.  

#### Caveats
In my email discussion with Andrey Karpov of PVS, he made the point that not all bugs are equal, and that a "checklist" approach to comparing static analyzers may not be the best.  I agree, but being able to compare analyzers on the same code-base can be very helpful, not least for getting a feel for how the tools work.

Your mileage can, and will, vary, so it makes sense to get comfortable with different tools and learn what each does best.  And there's no substitute for running the tools on your own code.  (The [helper scripts](/pages/REAME.md/index.html) ([repo here](https://github.com/btorpey/static)) may, well, help).

##### Specific issues 
The ITC test suite includes some tests for certain categories of errors that are more likely to manifest themselves at run-time, as opposed to compile-time.    

For instance, the ITC suite includes a relatively large number of test cases designed to expose memory-related problems.  These include problems like leaks, double-free's, dangling pointers, etc.

That's all very nice, but in the real world memory errors are often not that clear-cut, and depend on the dynamic behavior of the program at run-time.  Both valgrind's [memcheck](http://valgrind.org/info/tools.html#memcheck) and clang's [Address Sanitizer](http://clang.llvm.org/docs/AddressSanitizer.html) do an excellent job of detecting memory errors at run-time, and I use both regularly.

But run-time analyzers can only analyze code that actually runs, and memory errors can hide for quite a long time in code that is rarely executed (e.g., error & exception handlers). So, even though not all memory errors can be caught at compile-time, the ability to detect at least some of them can very helpful.  

A similar situation exists with regard to concurrency (threading) errors -- though in this case neither tool detects *any* of the concurrency-related errors seeded in the ITC code.  This is, I think, a reasonable design decision  -- the subset of threading errors that can be detected at compile-time is so small that it's not really worth doing (and could give users of the tool a false sense of security).  For concurrency errors, you again will probably be better off with something like clang's [Thread Sanitizer](http://clang.llvm.org/docs/ThreadSanitizer.html) or valgrind's [Data Race Detector](http://valgrind.org/info/tools.html#drd).

Also, in the interest of full disclosure, I have spot-checked some of the ITC code, but by no means all, to assure myself that its diagnostics were reasonable. 

With those caveats out of the way, though, the ITC test suite does provide at least a good starting point towards a comprehensive set of test cases that can be used to exercise different static analyzers.

The results of running PVS-Studio (and other tools) against the ITC code can be found in the [samples directory of the repo](https://github.com/btorpey/static/tree/master/samples).

## Real-world test results
I also ran both cppcheck and PVS-Studio on the code bases that I maintain as part of my day job, to get an idea of how the tools compare in more of a real-world situation.  While I can't share the detailed comparisons, following are some of the major points.

For the most part, both cppcheck and PVS-Studio reported similar warnings on the same code, with a few exceptions (listed following). 

cppcheck arguably does a better job of flagging "style" issues -- and while some of these warnings are perhaps a bit nit-picky, many are not:

- one-argument ctor's not marked `explicit` 
- functions that can/should be declared `static` or `const`
- use of post-increment on non-primitive types 
- use of obsolete or deprecated functions
- use of C-style casts

PVS-Studio, on the other hand, appears to include more checks for issues that aren't necessarily problems with the use of C++ per se, but things that would be a bug, or at least a "code smell", in any language.

A good example of that is PVS-Studio's warning on similar or identical code sequences (potentially indicating use of the copy-paste anti-pattern -- I've written about that [before](/blog/2014/09/21/repent/)).

Some other PVS-Studio "exclusives" include: 

- classes that define a copy ctor without `operator=`, and vice-versa
- potential floating-point problems[^float], e.g., comparing floating-point values for an exact match using `==`
- empty `catch` clauses
- catching exceptions by value rather than by reference

Both tools did a good job of identifying potentially suspect code, as well as areas where the code could be improved.

[^float]: See [here](http://blog.reverberate.org/2014/09/what-every-computer-programmer-should.html) and [here](http://floating-point-gui.de/) for an explanation of how floating-point arithmetic can produce unexpected results if you're not careful.

## False positives
False positives (warnings on code that is actually correct) are not really a problem with either cppcheck or PVS-Studio.  The few warnings that could be classified as false positives indicate code that is at the very least suspect -- in most cases you're going to want to change the code anyway, if only to make it clearer.

If you still get more false positives than you can comfortably deal with, or if you want to stick with a particular construct even though it may be suspect, both tools have mechanisms to suppress individual warnings, or whole classes of errors.  Both tools are also able to silence warnings either globally, or down to the individual line of code, based on inline comments.

## Conclusion
If you care about building robust, reliable code in C++ then you would be well-rewarded to include static analysis as part of your development work-flow.  

Both [PVS-Studio](http://www.viva64.com/en/pvs-studio/) and [cppcheck](http://cppcheck.sourceforge.net/) do an excellent job of identifying potential problems in your code.  It's almost like having another set of eyeballs to do code review, but with the patience to trace through all the possible control paths, and with a voluminous knowledge of the language, particularly the edge cases and "tricky bits".

Having said that, I want to be clear that static analysis is not a substitute for the dynamic analsyis provided by tools like valgrind's [memcheck](http://valgrind.org/info/tools.html#memcheck) and [Data Race Detector](http://valgrind.org/info/tools.html#drd), or clang's [Address Sanitizer](http://clang.llvm.org/docs/AddressSanitizer.html) and [Thread Sanitizer](http://clang.llvm.org/docs/ThreadSanitizer.html).  You'll want to use them too, as there are certain classes of bugs that can only be detected at run-time.

I hope you've found this information helpful.  If you have, you may want to check out some of my earlier articles, including:

- [Mo' Static](/blog/2016/04/07/mo-static/)
- [Static Analysis with clang](/blog/2015/04/27/static-analysis-with-clang/)
- [Using clang's Address Sanitizer](/blog/2014/03/27/using-clangs-address-sanitizer/)
- [Who Knows what Evil Lurks...](/blog/2015/03/17/shadow/)

Last but not least, please feel free to [contact me](<mailto:wallstprog@gmail.com>) directly, or post a comment below, if you have questions or something to add to the discussion.

## Appendix: Helper scripts and sample results

I've posted the [helper scripts](/pages/REAME.md/index.html) I used to run PVS-Studio, as well as the results of running those scripts on the ITC code, in the [repo](https://github.com/btorpey/static).

## Appendix: Detailed test results

The following sections describe a subset of the tests in the ITC code and how both tools respond to them.

### Bit Shift errors
{:.no_toc}
For the most part, PVS-Studio and cpphceck both do a good job of detecting errors related to bit shifts. Neither tool detects all the errors seeded in the benchmark code, although they miss different errors.

### Buffer overrun/underrun errors
{:.no_toc}
cppcheck appears to do a more complete job than PVS-Studio of detecting buffer overrrun and underrun errors, although it is sometimes a bit "off" -- reporting errors on lines that are in the vicinity of the actual error, rather than on the actual line.  cppcheck also reports calls to functions that generate buffer errors, which is arguably redundant, but does no harm.

PVS-Studio catches some of the seeded errors, but misses several that cppcheck detects.

While not stricly speaking an overrun error, cppcheck can also detect some errors where code overwrites the last byte in a null-terminated string.

### Conflicting/redundant conditions
{:.no_toc}
Both cppcheck and PVS-Studio do a good job of detecting conditionals that always evaluate to either true or false, with PVS-Studio being a bit better at detecting complicated conditions composed of contstants.

On the other hand, cppcheck flags redundant conditions (e.g., `if (i<5 && i<10)`), which PVS-Studio doesn't do. 

### Loss of integer precision
{:.no_toc}
Surprisingly, neither tool does a particularly good job of detecting loss of integer precision (the proverbial "ten pounds of bologna in a five-pound sack" problem ;-)

#### Assignments
{:.no_toc}
I say surprisingly because these kinds of errors would seem to be relatively easy to detect.  Where both tools seem to fall short is to assume that just because a value fits in the target data type, the assignment is valid -- but they fail to take into account that such an assignment can lose precision.

I wanted to convince myself that the ITC code was correct, so I pasted some of the code into a small test program:

<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span> (test1.c)</span> <a href='/downloads/code/static/pvs/test1.c'>download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="cp">#include &lt;stdio.h&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="kt">int</span> <span class="n">sink</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="kt">void</span> <span class="nf">data_lost_001</span> <span class="p">()</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>	<span class="kt">char</span> <span class="n">ret</span><span class="p">;</span>
</span><span class='line'>	<span class="kt">short</span> <span class="n">a</span> <span class="o">=</span> <span class="mh">0x80</span><span class="p">;</span>
</span><span class='line'>	<span class="n">ret</span> <span class="o">=</span> <span class="n">a</span><span class="p">;</span><span class="cm">/*Tool should detect this line as error*/</span> <span class="cm">/*ERROR:Integer precision lost because of cast*/</span>
</span><span class='line'>        <span class="n">sink</span> <span class="o">=</span> <span class="n">ret</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>   <span class="n">data_lost_001</span><span class="p">();</span>
</span><span class='line'>   <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Value of sink=%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">sink</span><span class="p">);</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

When you run this program, you'll get the following output:

    $ gcc test1.c && ./a.out
    Value of sink=-128

So, `a` has the value 128, but when `a` is assigned to the (signed) char `ret`,  the bit pattern `0x80` is interpreted in the context of a (signed) char, and the sign is lost.  If `ret` had been declared as an unsigned char, then the assigment would not lose the sign of `a`.

#### Arithmetic expressions
{:.no_toc}
cppcheck does do a slightly better job of detecting integer overflow and underflow in arithmetic expressions compared to PVS, but still misses a number of seeded errors.

#### Divide by zero
{:.no_toc}
Both PVS-Studio and cppcheck do a good job of catching potential divide-by-zero errors, with cppcheck having a slight edge. 

### Dead code
{:.no_toc}
PVS-Studio tends to do a somewhat better job than cppcheck at detecting various types of dead code, such as `for` loops and `if` statements where the condition will never be true.

PVS-Studio also very helpfully flags any unconditional `break` statements in a loop -- these are almost always going to be a mistake.

### Concurrency
{:.no_toc}
As mentioned above, neither tool detects *any* of the concurrency-related errors seeded in the ITC code.  Again, I regard that as a reasonable design choice, given the relatively small percentage of such errors that can be detected at compile-time.

### Memory Errors
{:.no_toc}
As discussed earlier, not all memory errors can be detected at compile-time, so the lack of any error output certainly doesn't mean that the code doesn't have memory errors -- it just means that they can't be detected by the tools. But while many memory errors cannot be detected at compile-time, for those that can be, detecting them is a big win.

#### Double free
{:.no_toc}
cppcheck does an excellent job of detecting double-free errors (11 out of 12), while PVS-Studio only flags one of the seeded errors.

#### Free-ing non-allocated memory
{:.no_toc}
On the other hand, PVS-Studio does a better job of detecting attempts to free memory that was not allocated dynamically (e.g., local variables).  

#### Freeing a NULL pointer
{:.no_toc}
Neither tool does a particularly good job of catching these.  Perhaps that is because freeing a NULL pointer is actually not an error, but doing so is certainly a clue that the code may have other problems.

#### Dangling pointers
{:.no_toc}
cppcheck does a somewhat better job of detecting the use of dangling pointers (where the pointed-to object has already been freed).

#### Allocation failures
{:.no_toc}
If you're writing code for an embedded system, then checking for and handling allocation failures can be important, because your application is likely written to expect them, and do something about them.  But more commonly, running out of memory simply means that you're screwed, and attempting to deal with the problem is unlikely to make things better.

Neither tool detects code that doesn't handle allocation failures, but cppcheck does flag some allocation-related problems (as leaks, which is not correct, but it is a clue that there is a problem lurking).

#### Memory Leaks
{:.no_toc}
Typically, memory leaks are only evident at run-time, but there are some cases where they can be detected at compile-time, and in those cases cppcheck does a pretty good job. 

#### Null pointer
{:.no_toc}
Both PVS-Studio and cppcheck do a good job of flagging code that dereferences a NULL pointer, although neither tool catches all the errors in the benchmark code.

#### Returning a pointer to a local variable
{:.no_toc}
Both PVS-Studio and cppcheck detect returning a pointer to a local variable that is allocated on the stack.

#### Accessing un-initialized memory
{:.no_toc}
PVS-Studio does a somewhat better job than cppcheck of flagging accesses to uninitialized memory.

### Infinite loops
{:.no_toc}
Both cppcheck and PVS-Studio detect some infinite loop errors, but miss several others.  It could be that this is by design, since the code that is not flagged tends to resemble some idioms (e.g., ` while (true)`) that are often used deliberately.  

### Ignored return values
{:.no_toc}
PVS-Studio is quite clever here -- it will complain about an unused return value from a function, *if* it can determine that the function has no side effects.  It also knows about some common STL functions that do not have side effects, and will warn if their return values are ignored.

cppcheck doesn't check for return values per se, but it will detect an assignment that is never referenced.  This makes some sense, since warning on ignored return values could result in a large number of false positives.

### Empty/short blocks
{:.no_toc}
Both tools detect certain cases of empty blocks (e.g., `if (...);` -- note the trailing semi-colon).  

What neither tool does is warn about "short" blocks -- where a conditional block is not enclosed in braces, and so it's not 100% clear whether the conditional is meant to cover more than one statement:


    if (...)
       statement1();
       statement2();

If you've adopted a convention that even single-statement blocks need to be enclosed in braces, then this situation may not pertain (and good for you!).  Still, I think this would be a worthwhile addition -- at least in the "style" category.

### Dead stores
{:.no_toc}
cppcheck does a particularly good job of detecting dead stores (where an assignment is never subsequently used).  PVS-Studio, on the other hand, flags two or more consecutive assignments to a variable, without an intervening reference.  PVS-Studio will also flag assignment of a variable to itself (which is unlikely to be what was intended).
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Custom-Tailored Configuration]]></title>
    <link href="http://btorpey.github.io/blog/2016/10/13/custom-tailor/"/>
    <updated>2016-10-13T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2016/10/13/custom-tailor</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/customtailor.jpg">

As developers, we seem to take a special delight in personalizing the virtual worlds in which we work -- from color palettes to keyboards, fonts, macros, you name it.  "Off-the-rack" is never good enough, we want Saville Row tailoring for our environments.

And a lot of the tools we use support and encourage that customization, giving us control over every little option.

But not every tool we use does so -- read on to learn a very simple trick to how to take control even when your tool doesn't make that easy.

<!-- more -->

In Linux, we have a couple of common ways to customize the way our tools work -- by defining environment variables, and by using configuration files.  Sometimes these two mechanisms work well together, and we can include environment variables in configuration files to make them flexible in different situations.

Not every tool can expand environment varaiables in its configuration files, however.  In that case, you can use this simple Perl one-liner to subsitute values from the environment into any plain-text file.

    perl -pe '$_=qx"/bin/echo -n \"$_\"";' < sample.ini


What's happpening here is

The `-p` switch tells Perl to read every line of input and print it.

The `-e` switch tells Perl to execute the supplied Perl code against every line of input.

The code snippet replaces the value of the input line (`$_`) with the results of the shell command specified by the `qx` function.  That shell command simply echos[^echo] the value of the line (`$_`), but it does so inside double quotes (the `\"`), which causes the shell to replace any environment variable with its value.

[^echo]: Note that we use /bin/echo here, instead of just plain echo, to get around an issue with the echo command in BSD (i.e., OSX).

And that's it!  Since the subsitution is being done by the shell itself, you can use either form for the environment variable (either `$VARIABLE` or `${VARIABLE}`), and the replacement is always done using the rules for the current shell.

Here's an example -- let's create a simple .ini type file, like so:

    username=$USER
    host=$HOSTNAME
    home-directory=$HOME
    current-directory=$PWD

When we run this file through our Perl one-liner, we get:

    perl -pe '$_=qx"/bin/echo -n \"$_\"";' < sample.ini
    username=btorpey
    host=btmac
    home-directory=/Users/btorpey
    current-directory=/Users/btorpey/blog/code/tailor

One thing to watch out for is that things can get a little hinky if your input file contains quotes, since the shell will interpret those, and probably not in the way you intend.  At least in my experience, that would be pretty rare -- but if you do get peculiar output that would be something to check.
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mo' Static]]></title>
    <link href="http://btorpey.github.io/blog/2016/04/07/mo-static/"/>
    <updated>2016-04-07T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2016/04/07/mo-static</id>
    <content type="html"><![CDATA[<p><img class="left" src="/images/nye-static.jpg" width="240" height="180">

- Will be replaced with the ToC, excluding the "Contents" header
{:toc}

In my day job, one of my main focuses is software reliability and correctness, so it makes sense that I would be a big fan of static analysis.

I've written previously about the [static analysis provided by clang](/blog/2015/04/27/static-analysis-with-clang/).  Today, I want to take a bit of a "deep-dive" into the whole subject by putting both clang and [cppcheck](http://cppcheck.sourceforge.net/) through their paces, using them to analyze a benchmark suite designed to exercise static analysis tools.  In the course of doing that, I'll also provide some helper scripts that make working with the tools easier.  

<!-- more -->

## Testing Static Analysis Tools

>  And what is good Phaedrus, and what is not good -- need we ask anyone to tell us these things? [^zen]  

[^zen]: Robert Pirsig, "Zen and the Art of Motorcycle Maintenance"

Obviously, the ultimate goal is to be able to run static analysis tools against our own codebase(s) to help detect and fix problems.  But how do we know if a particular tool is actually finding problems?  And, how do we know if we're running the tool properly?  

The perfect static analyzer would find all the latent bugs in our code, while not reporting any false positives[^fpos].  Since there are no perfect analyzers, any tool we use is going to miss some errors, and/or wrongly flag correct code.  So, the only way to evaluate an analyzer is to know where all the bugs are in our code -- but if we knew that, we wouldn't need an analyzer.

[^fpos]: A "false positive" is when a tool reports an error that is actually not.

That's a dilemma. To resolve it, we're going to be using a codebase specifically designed to trigger static analysis warnings.  The code was originally developed by Toyota ITC, and is available on [John Regehr's excellent blog](http://blog.regehr.org/archives/1217).  

The ITC benchmarks attempt to resolve our dilemma by providing both a set of code that contains errors which *should* trigger warnings, as well as a second set of code, similar to the first, but which doesn't contain errors.  Each source file is annotated with comments documenting where the errors are (and aren't).  And that lets us create a catalog of both real errors and potential false positives[^disclaimer]. 

To get started, download the code from [its GitHub repository](https://github.com/regehr/itc-benchmarks), and set the `ITCBENCH_ROOT` environment variable (which will come in handy later):

    $ git clone https://github.com/regehr/itc-benchmarks
    $ export ITCBENCH_ROOT=$(pwd)/itc-benchmarks

## Can I haz teh codez?
The remainder of this article goes step-by-step through the process of creating a compilation database from the ITC benchmark code, running clang's static analysis tools against that compilation database, building and installing cppcheck and running it against the compilation database, and analyzing the results.

This is all good stuff, especially if you're going to be using these tools going forward.  But, there's a certain amount of unavoidable yak-shaving[^yak] going on to get to that point.  So if you prefer to skip all that, I've included the results of running the different tools in the samples directory of the [repo](https://github.com/btorpey/static).  The samples include all the files we're going to be generating the hard way, so you can follow along without all the requisite busy-work.  Hopefully, when we're done you'll want to go back and use these tools on your own codebase. 

[^yak]: See <https://en.wiktionary.org/wiki/yak_shaving> for a description of this colorful term.

## Creating a compilation database
To run both clang and cppcheck we first need to create a "compilation database" to supply them with required build settings.  The [compilation database](http://clang.llvm.org/docs/JSONCompilationDatabase.html) format was developed as part of the clang project, to provide a way for different tools to query the actual options used to build each file.

A [good overview of how the compilation database works](http://eli.thegreenplace.net/2014/05/21/compilation-databases-for-clang-based-tools) with clang-based tools can be found at Eli Bendersky's excellent site.  His article illustrates the importance of making sure that code analysis tools are looking at the same (pre-processed) source that the actual compiler sees, in order to generate meaningful diagnostics with a minimum of false positives.

- If you are using [cmake](http://cmake.org/) to drive your builds, creating a compilation database couldn't be easier -- simply add the `-DCMAKE_EXPORT_COMPILE_COMMANDS=ON` parameter to the cmake build command, or add the following to your main CMakeLists.txt file:

    `set(CMAKE_EXPORT_COMPILE_COMMANDS ON)`

- If you're not using cmake, you can still create a compilation database using plain old make by front-ending make with [Bear](<https://github.com/rizsotto/Bear>)[^bear], like so:

    `bear make`

In either case, the end result should be the creation of a  `compile_commands.json` file in the current directory.

Sadly, the ITC benchmark suite is stuck in the past using [autotools](https://twitter.com/timmartin2/status/23365017839599616), and worse yet, a version that needs to be installed from source (on RH6, at least).     

So, in the interest of immediate gratification, I've included the compile\_commands.json file [here](/downloads/code/static/samples/compile_commands.json) -- simply save it to the directory where you've cloned the ITC code.  (The compile_commands.json file is also contained in the samples directory of the [repo for this article](https://github.com/btorpey/static)).

If you prefer to generate the compile_commands.json file yourself using Bear, you can do so like this:

    $ cd ${ITCBENCH_ROOT}  
    $ ./bootstrap
    $ ./configure
    $ bear make

[^bear]: Building and installing Bear from source is relatively straightforward -- just keep in mind that you need python >= 2.7.

## Establishing a baseline
To make it possible to compare results from different analyzers, we first need to establish a baseline using the ITC benchmarks, and for that we're going to need [this set of helper scripts](/pages/REAME.md/index.html), which can be downloaded from [this GitHub repo](https://github.com/btorpey/static).

    $ git clone https://github.com/btorpey/static
    
Once you've done that, you need to add the directory to your PATH:

    $ export PATH=$(pwd)/static/scripts:$PATH 

Enter the following command from the ITC source directory to create a csv file with the error annotations from the ITC code:

    $ cd ${ITCBENCH_ROOT}  
    $ cc_driver.pl -n grep -Hni ERROR: | 
    itc2csv.pl -r ${ITCBENCH_ROOT}/ | 
    sort -u > itc.csv  


The command will create a file named `itc.csv` in the source directory that looks like this:

    $ cat itc.csv
    "01.w_Defects/bit_shift.c:106","/*ERROR:Bit shift error*/"
    "01.w_Defects/bit_shift.c:120","/*ERROR:Bit shift error*/"
    "01.w_Defects/bit_shift.c:133","/*ERROR:Bit shift error*/"
    "01.w_Defects/bit_shift.c:146","/*ERROR:Bit shift error*/"
    "01.w_Defects/bit_shift.c:163","/*ERROR:Bit shift error*/"
    "01.w_Defects/bit_shift.c:175","/*ERROR:Bit shift error*/"
    ...

The format of the csv file is really simple -- just an entry for file and line number, and another with the error annotation munged from the source file.  This will give us a baseline against which to compare both clang and cppcheck.


## Using clang's analysis tools

In a couple of previous posts, I wrote about [static analysis with clang](/blog/2015/04/27/static-analysis-with-clang), and [how to build clang](/blog/2015/01/02/building-clang).  This next bit assumes that you've got clang ready-to-go, but if that's not the case, there can be a fair amount of work required to get to that point, so you may want to skip ahead to the section on [using cppcheck](#using-cppcheck).

We're going to use a similar approach to the one we used above to generate the list of expected errors from the ITC code.  The command below will run clang-check against all the files in compile_commands.json, filter the results, and reformat the output in csv format:

    $ cd ${ITCBENCH_ROOT}  
    $ cc_driver.pl clang-check -analyze 2>&1 | 
    clang2csv.pl -r ${ITCBENCH_ROOT}/ |
    sort -u > clangcheck.csv

This gives us the diagnostic messages produced by clang, in the same csv format as we used for the list of errors, above: 

    $ cat clangcheck.csv
    "01.w_Defects/bit_shift.c:106","warning: The result of the '<<' expression is undefined"
    "01.w_Defects/bit_shift.c:133","warning: The result of the '<<' expression is undefined"
    "01.w_Defects/bit_shift.c:146","warning: The result of the '<<' expression is undefined"
    "01.w_Defects/bit_shift.c:163","warning: The result of the '<<' expression is undefined"
    "01.w_Defects/bit_shift.c:175","warning: The result of the '<<' expression is undefined"
    ...

We can already see that there are some differences: the ITC code expects to see a diagnostic at 01.w_Defects/bit_shift.c:120, but clang doesn't output a warning for that line.

### Analyzing the results

What I like to do at this point is fire up my all-time favorite tool, [Beyond Compare](/blog/2013/01/29/beyond-compare/), to generate a visual diff of the two files:

<img class="center" src="/images/itcvsclang.png"> 

This view shows the expected diagnostics extracted from the ITC source files on the left, alongside the diagnostics generated by clang on the right.  We can see that clang catches some of the bugs in the source file, but misses others.  If we continue to read down the two files, we'll also see some potential "false positives" -- i.e., diagnostics issued by clang that are not marked as expected errors in the source files. 

The visual approach using Beyond Compare works well for me, but with a csv-formatted datafile, other approaches are possible as well.  We could import the diagnostic messages into a spreadsheet program, or even a DBMS, for archiving, tracking and comparison. 

## Running clang analysis (again)
clang actually has two tools for doing static analysis -- in the example above we ran `clang-check -analyze`, but now we're going to use `clang-tidy` instead.

    $ cd ${ITCBENCH_ROOT}  
    $ cc_driver.pl clang-tidy 2>&1 | 
    clang2csv.pl -r ${ITCBENCH_ROOT}/ | 
    sort -u > clangtidy.csv

If you compare the results from clang-check and clang-tidy, you'll notice that clang-tidy generally reports more warnings than clang-check.  Some of them are not necessarily defects, but are arguably bad practice (e.g., using `strcpy`).

<img class="center" src="/images/clangcheckvstidy.png"> 

clang-tidy also outputs a slightly different format, including the name of the check in brackets.  (The name can also be used to suppress the warning).

The choice of which to use is up to you -- my preference is to use clang-check first, and follow up with clang-tidy, simply because the warnings produced by clang-tidy either duplicate those from clang-check, or are not as serious.

Note that you can get a list of available checks from clang with the following command:

    $ clang -cc1 -analyzer-checker-help
    ...
    core.DivideZero                 Check for division by zero
    core.DynamicTypePropagation     Generate dynamic type information
    core.NonNullParamChecker        Check for null pointers passed as arguments to a function whose arguments are references or marked with the 'nonnull' attribute
    core.NullDereference            Check for dereferences of null pointers
    core.StackAddressEscape         Check that addresses to stack memory do not escape the function


## Using cppcheck

There's another static analysis tool that can provide results comparable to clang.  [cppcheck](http://cppcheck.sourceforge.net/) has been around for a while, and I had tried to get it working in the past, but had given up after bumping into a few problems.

I kept hearing good things about cppcheck in [articles and presentations by others](#references), though, so I finally decided it would be worth the trouble to get it working.

It turns out the problems were not that difficult to solve, given a combination of documentation and experimentation.  And the benefits were significant, so I'm quite happy to have added cppcheck to my tool box.

### Installing cppchceck
While cppcheck is available bundled with some distros, it's often an older version, so we're going to build and install it from source. As is more and more often the case, cppcheck has started using features of C++1x, so we're going to need a C++1x-capable compiler to build it.

If you're on an older distro (in my case, RH6) where the system compiler is not C++1x-capable, see my [earlier post](/blog/2015/01/02/building-clang/) about how to build clang (and/or gcc) to get a C++1x-capable compiler.  (Basically, it uses an older version of gcc to build a newer version, and the newer version to build clang).  

It took some trial-and-error to get the cppcheck build parameters right, but the [supplied build script](/pages/build_cppcheck.sh/index.html) should get the job done[^install].

    $ ./build_cppcheck.sh 2>&1 | tee build_cppcheck.out

[^install]: As usual, I prefer installing external packages in a non-standard location, so the build script is set up to do that.  See [this post](/blog/2015/01/02/building-clang/) for an explanation and rationale of this approach).

#### Verifying the installation
You'll need to add the cppcheck directory to your PATH (assuming the install location from the build script):

    $ export PATH=/build/share/cppcheck/1.73/bin:$PATH

If the build and install process worked, you should be able to invoke cppcheck from the command line, like so:

    $ cppcheck --version
    Cppcheck 1.73
 
If you see the message below instead, there's a problem with the RPATH setting:
   
    $ cppcheck --version
    cppcheck: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.15' not found (required by cppcheck)

The problem is typically either that the RPATH setting in the build is incorrect, or that the directory referenced by the RPATH setting does not exist.

### Running cppcheck

Now we're ready to run cppcheck, using the same approach we used with clang: 

    $ cd ${ITCBENCH_ROOT}  
    $ cc_driver.pl cppcheck.sh  2>&1 | 
    cppcheck2csv.pl -r ${ITCBENCH_ROOT} | 
    sort -u > cppcheck.csv

Note that instead of invoking cppcheck directly, we're invoking it via the [cppcheck.sh](/pages/cppcheck.sh/index.html) helper script, which supplies needed parameters to cppcheck.  It also creates an include file with the compiler's pre-defined macros, so those definitions will be visible to cppcheck.  This turns out to be particularly important with cppcheck, especially if the code you're trying to analyze uses `#ifdef`'s to control what code actually gets compiled (or seen by cppcheck)[^nosys].

[^nosys]: Note that cppcheck does not particularly like it when you include system include directories using `-I`.  Accordingly, we don't pass the `-s` switch to  cc_driver.pl when running cppcheck.

One of the settings in the helper script enables what cppcheck calls "inconclusive" results.  These are exactly what the name implies -- cppcheck isn't positive that the code is wrong, but it is at least suspicious.  Including these inconclusive results should tend to increase the number of false positives in theory, but in practice I haven't found false positives to be a big problem with either cppcheck or clang.  

### Analyzing the results
One of the first things you notice with cppcheck is that it includes more checks than clang.  Some of the additional warnings are for constructs that are not exactly *wrong*, but are either non-optimal, or indicators of potential problems.  For instance, cppcheck will warn when a variable is defined in a broader scope than is actually required ("scope ... can be reduced").  

<img class="center" src="/images/itcvscppcheck.png"> 

You can get a list of all the checks cppcheck is performing like so:

    $ cppcheck --doc 
    ...
    ## Other ##
    Other checks
    - division with zero
    - scoped object destroyed immediately after construction
    - assignment in an assert statement
    - free() or delete of an invalid memory location
    - bitwise operation with negative right operand
    - provide wrong dimensioned array to pipe() system command (--std=posix)


You can also generate a list of error ID's with this command:

    $ cppcheck --errorlist
    <error id="stringLiteralWrite" severity="error" msg="Modifying string literal directly or indirectly is undefined behaviour."/>
    <error id="sprintfOverlappingData" severity="error" msg="Undefined behavior: Variable &apos;varname&apos; is used as parameter and destination in s[n]printf()."/>
    <error id="strPlusChar" severity="error" msg="Unusual pointer arithmetic. A value of type &apos;char&apos; is added to a string literal."/>
    <error id="incorrectStringCompare" severity="style" msg="String literal &quot;Hello World&quot; doesn&apos;t match length argument for substr()."/>
    <error id="literalWithCharPtrCompare" severity="style" msg="String literal compared with variable &apos;foo&apos;. Did you intend to use strcmp() instead?"/>
    <error id="charLiteralWithCharPtrCompare" severity="style" msg="Char literal compared with pointer &apos;foo&apos;. Did you intend to dereference it?"/>
    <error id="incorrectStringBooleanError" severity="style" msg="Conversion of string literal &quot;Hello World&quot; to bool always evaluates to true."/>
 

You can suppress any errors you don't care to see by passing its id in the `--suppress=` flag.

## Comparing clang and cppcheck
There's a school of thought that says you should use as many compilers as possible to build your code, because each one will find different problems.  That's still a good idea, and even more so with static analysis tools.  

There's a certain amount of overlap between clang and cppcheck, but there are also significant differences.  In my experience, if clang reports something as a problem, it almost certainly is one, but clang also misses a lot of problems that it could detect.

<img class="center" src="/images/clangvscppcheck.png"> 

cppcheck can generate more warnings, and some of them are more stylistic issues, but it does detect certain classes of problems, like dead code and arithmetic over/underflow, that clang doesn't.

As I mentioned earlier, I haven't found false positives to be a major problem with either clang or cppcheck.

So, each tool has its place, and I like to use both.

# Conclusions
Static analysis tools can add real value to the software development process by detecting errors, especially errors in code that is never or almost never executed.

Commercial tools can be expensive (although still cheap compared to the money they save), and open-source tools can sometimes be hard to use (or at least hard to learn how to use).

The provided [helper scripts](/pages/REAME.md/index.html) ([repo here](https://github.com/btorpey/static)) should make it much easier to use these tools, and to keep track of warnings and compare the outputs of different tools by using a common format.

They can also be useful for before-and-after comparisions of different versions of a single codebase -- for example, as changes are being made to address issues detected by the tools.

# Acknowledgements
In addition to the people, projects and organizations mentioned earlier, the people at the NIST have been very helpful, and maintain an incredible collection of resources on the topic of static analysis for a number of languages, not just C++.  Some of those resources include the following, and are well worth checking out:

<https://samate.nist.gov/index.php/SAMATE_Publications.html>  
<https://samate.nist.gov/SARD/>  

If you've read any of my other posts, you may have noticed that the contents sidebar at the beginning of the article is a new thing.  Especially for longer-format articles, that TOC would seem to be very helpful.  Many thanks to [Robert Riemann](http://blog.riemann.cc/2013/04/10/table-of-contents-in-octopress/) for taking the trouble to explain how to do it.

I've been using the very nice [MacDown](http://macdown.uranusjr.com/) editor to create these posts -- thanks, Tzu-Ping!

# References
Some helpful references that I ran across while researching this article:

[Static Code Analysis, John Carmack](http://www.viva64.com/en/a/0087/)

[CppCon 2015: Jason Turner ‚ÄúThe Current State of (free) Static Analysis"](https://youtu.be/sn1Vg8A_MPU)

[CppCon 2015: Neil MacIntosh ‚ÄúStatic Analysis and C++: More Than Lint"](https://youtu.be/rKlHvAw1z50)

---

[^disclaimer]: Full disclaimer: I have not taken the time to review all of the ITC source to verify that the annotations are accurate and/or complete.  For the purpose of this exercise, we'll agree to assume that they are -- but if you'd like to suggest any improvements, I'm guessing the best place to do that would on the [repo](https://github.com/regehr/itc-benchmarks).

</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Remote Scripting with bash and ssh]]></title>
    <link href="http://btorpey.github.io/blog/2015/10/13/remote-scripting-with-bash-and-ssh/"/>
    <updated>2015-10-13T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2015/10/13/remote-scripting-with-bash-and-ssh</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/multimonitors.jpg" width="370" height="245">

Nowadays it's pretty common for applications to be distributed across multiple
machines, which can be good for scalability and resilience.

But it does mean that we have more machines to monitor -- sometimes a LOT more!

Read on for a handy tip that will let you do a lot of those tasks from any old
session (and maybe lose some of those screens)!

<!-- more -->

For really simple tasks, remote shell access using ssh is fine.  But oftentimes
the tasks we need to perform on these systems are complicated enough that they
really should be scripted.

And especially when under pressure, (e.g.,  troubleshooting a problem in a
production system) it's good for these tasks to be automated. For one thing,
that means they can be tested ahead of time, so you don't end up doing the
dreaded `rm -rf *` by mistake.  (Don't laugh -- I've actually seen that happen).

Now, I've seen people do this by copying scripts to a known location on the
remote machines so they can be executed.  That works, but has some
disadvantages: it clutters up the remote system(s), and it creates one more
artifact that needs to be distributed and managed (e.g., updated when it
changes).

If you've got a bunch of related scripts, then you're going to have to bite the
bullet and manage them (perhaps with something like Puppet).

But for simple tasks, the following trick can come in very handy:

`
ssh HOST ‚Äòbash ‚Äìs ‚Äò < local_script.sh
`

What we're doing here is running bash remotely and telling bash to get its input
from stdin.  We're also redirecting local_script.sh to the stdin of ssh, which
is what the remote bash will end up reading.

As long as local_script.sh is completely self-contained, this works like a
charm.

For instance, to login to a remote machine and see if hyper-threading is enabled
on that machine:

`
ssh HOST 'bash -s' < ht.sh
`

Where ht.sh looks like this:

<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span> (ht.sh)</span> <a href='/downloads/code/bash/ht.sh'>download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/bin/bash</span>
</span><span class='line'>
</span><span class='line'><span class="c"># cribbed from http://unix.stackexchange.com/questions/33450/checking-if-hyperthreading-is-enabled-or-not</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># NOTE:  There does not seem to be a good way to determine if HT is available but not enabled on a particular machine:</span>
</span><span class='line'><span class="c"># - &#39;ht&#39; flag in /proc/cpuinfo is unreliable</span>
</span><span class='line'><span class="c"># - lscpu could be used, but is not part of RH5</span>
</span><span class='line'><span class="c"># - dmidecode could be used, but requires root permissions</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># So for now we just report whether HT is enabled or not</span>
</span><span class='line'>
</span><span class='line'><span class="nb">echo</span> -n <span class="k">${</span><span class="nv">HOSTNAME</span><span class="k">}</span>
</span><span class='line'>
</span><span class='line'><span class="nv">nproc</span><span class="o">=</span><span class="k">$(</span>grep -i <span class="s2">&quot;processor&quot;</span> /proc/cpuinfo | sort -u | wc -l<span class="k">)</span>
</span><span class='line'><span class="nv">phycore</span><span class="o">=</span><span class="k">$(</span>cat /proc/cpuinfo | egrep <span class="s2">&quot;core id|physical id&quot;</span> | tr -d <span class="s2">&quot;\n&quot;</span> | sed s/physical/<span class="se">\\</span>nphysical/g | grep -v ^<span class="nv">$ </span>| sort -u | wc -l<span class="k">)</span>
</span><span class='line'><span class="k">if</span> <span class="o">[</span> -z <span class="s2">&quot;$(echo &quot;</span><span class="nv">$phycore</span> *2<span class="s2">&quot; | bc | grep $nproc)&quot;</span> <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">   </span><span class="nb">echo</span> <span class="s2">&quot;: HT disabled&quot;</span>
</span><span class='line'><span class="k">else</span>
</span><span class='line'><span class="k">   </span><span class="nb">echo</span> <span class="s2">&quot;: HT enabled&quot;</span>
</span><span class='line'><span class="k">fi</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>


(The script above was cribbed from http://unix.stackexchange.com/a/33509 --
thanks, Nils!)


Of course, all the normal redirection rules apply -- you just have to keep in
mind that you're redirecting to ssh, which is then redirecting to bash on the
input side.  On the output side, it's reversed.

Give this a try the next time you need to do some quick tasks over ssh and
you'll be able to get rid of a few of those monitors!
</p>
]]></content>
  </entry>
  
</feed>
