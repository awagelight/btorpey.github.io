<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Confessions of a Wall Street Programmer]]></title>
  <link href="http://btorpey.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://btorpey.github.io/"/>
  <updated>2014-04-29T21:12:06-04:00</updated>
  <id>http://btorpey.github.io/</id>
  <author>
    <name><![CDATA[Bill Torpey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Picture is Worth 1K Words]]></title>
    <link href="http://btorpey.github.io/blog/2014/04/29/a-picture-is-worth-1k-words/"/>
    <updated>2014-04-29T19:23:07-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/04/29/a-picture-is-worth-1k-words</id>
    <content type="html"><![CDATA[<p>You know those mutiple-choice tests that put you in one of four quadrants based
on your answers to a bunch of seemingly irrelevant questions? We've all taken
them, and if you're like me they're kind of like reading your horoscope -- it
all seems so right and true when you're reading it, but you wonder if it would
still seem just as right and true if the horoscopes got jumbled at random?

Well, I took one of these a while back that wasn't like that -- it was the "Learning-Style Inventory"
test, and what it said about me is that I'm waaaayyy over at the end of the
scale when it comes to visual thinking. That gave me an insight into the way my
brain works that I've found really helpful since. So, this next bit was right up my alley,
but I'm guessing you'll like it too.

We read a lot lately about NUMA architecture and how it presents a fundamental
change in the way we approach writing efficient code: it's no longer about the
CPU, it's all about RAM. We all nod and say "Sure, I get that!"  Well, I thought
I got it too, but until I saw [this web page](<http://www.overbyte.com.au/misc/Lesson3/CacheFun.html>), 
I really didn't. 

<a href=http://www.overbyte.com.au/misc/Lesson3/CacheFun.html><img src=http://overbyte.com.au/wp-content/uploads/2012/01/InteractiveMemAccess-620x424.png></a>

See the full discussion at <http://overbyte.com.au/index.php/overbyte-blog/entry/optimisation-lesson-3-the-memory-bottleneck>.
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using clang's Address Sanitizer (without clang)]]></title>
    <link href="http://btorpey.github.io/blog/2014/03/27/using-clangs-address-sanitizer/"/>
    <updated>2014-03-27T08:30:22-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/03/27/using-clangs-address-sanitizer</id>
    <content type="html"><![CDATA[<p>Valgrind has been an indispensable tool for C/C++ programmers for a long
time, and I've used it quite happily -- it's a tremendous tool for doing dynamic
analysis of program behavior at run time. valgrind[^3] can detect reads of
uninitialized memory, heap buffer overruns, memory leaks, and other errors that
can be difficult or impossible to find by eyeballing the code, or by static
analysis tools.  But that comes with a price, which in some cases can be quite steep, and some new
tools promise to provide some or all of the functionality valgrind provides without the drawbacks.

<!--more-->

For one thing, valgrind can
be *extremely* slow.  That is an unavoidable side-effect of one of valgrind's
strengths, which is that it doesn't require that the program under test be
instrumented beforehand -- it can analyze any executable (including shared
objects) "right out of the box".  That works because valgrind effectively
emulates the hardware the program runs on, but that leads to a potential
problem: valgrind instruments *all* the code, including shared objects --and
that includes third-party code (e.g., libraries, etc.) that you may not have any
control over.

In my case, that ended up being a real problem.  The main reason
being that a significant portion of the application I work with is hosted in a
JVM (because it runs in-proc to a Java-based FIX engine, using a thin JNI
layer).  The valgrind folks say that the slowdown using their tool can be up to
20x, but it seemed like more, because the entire JVM was being emulated.

And, because valgrind emulates *everything*, it also detects and reports
problems in the JVM itself.  Well, it turns out that the JVM plays a lot of
tricks that valgrind doesn't like, and the result is a flood of complaints that
overwhelm any potential issues in the application itself.

So, I was very interested in learning about a similar technology that promised
to address some of these problems.  Address Sanitizer (Asan from here on) was
originally developed as part of the clang project, and largely by folks at Google.
They took a different approach: while valgrind emulates the machine at run-time, Asan works by instrumenting
the code at compile-time.

That helps to solve the two big problems that I was having with valgrind: its
slowness, and the difficulty of excluding third-party libraries from the
analysis.

Asan with clang
---------------

Since I was already building the application using clang for its excellent
diagnostics and static analysis features, I thought it would be relatively
straightforward to introduce the Asan feature into the build.  Turns out there
is a bump in that road: clang's version of Asan is supplied only as a
static library that is linked into the main executable.  And while it should be
possible to re-jigger things to make it work as a shared library, that would
turn into a bit of science project.  That, and the fact that the wiki page discussing it
(http://code.google.com/p/address-sanitizer/wiki/AsanAsDso) didn't sound
particularly encouraging ("however the devil is in the detail" -- uhh, thanks, no).

Rats!  However, the wiki page
did mention that there was a version of Asan that worked with gcc, and that
version apparently did support deployment as a shared object.  So, I decided to give that a try...

Asan with gcc
-------------

It turns out that the gcc developers haven't been sitting still -- in
fact, it looks like there is a bit of a healthy rivalry between the clang and gcc
folks, and that's a good thing for you and me.  Starting with version 4.8 of the
gcc collection, Asan is available with gcc as well.[^2]

Getting the latest gcc version (4.8.2 as of this writing), building and
installing it was relatively straight-forward.  By default, the source build
installs into /usr/local, so it can co-exist nicely with the native gcc for the
platform (in the case of Red Hat/CentOS 6.5, that is the relatively ancient gcc
4.4 branch).

Building with Asan
-------------
Including support for Asan in your build is pretty simple -- just include the `-fsanitize=address`
flag in both the compile and link step.  (Note that this means you need to invoke the linker via the compiler
driver, rather than directly.  In practice, this means that the executable you specify for the link step should be 
g++ (or gcc), not ld).  

While not strictly required, it's also a very good idea to include the `-fno-omit-frame-pointer` flag
in the compile step.  This will prevent the compiler from optimizing away the frame pointer (ebp) register.  While
disabling any optimization might seem like a bad idea, in this case the performance benefit is likely minimal at best[^5], but the 
inability to get accurate stack frames is a show-stopper.

Running with Asan
-------------
If you're checking an executable that you build yourself, the prior steps are all you need -- libasan.so will get linked
into your executable by virtue of the `-fsanitize=address` flag.

In my case, though, the goal was to be able to instrument code running in the JVM.  In this case, I had to force libasan.so
into the executable at runtime using `LD_PRELOAD`, like so:

`LD_PRELOAD=/usr/local/lib64/libasan.so.0 java ...`

And that's it!

Tailoring Asan
---------------

There are a bunch of options available to tailor the way Asan works: at compile-time you can supply a "blacklist" of functions that
Asan should NOT instrument, and at run-time you can further customize Asan using the `ASAN_OPTIONS` environment variable, which
is discussed [here](<http://code.google.com/p/address-sanitizer/wiki/Flags>).
 
By default, Asan is silent, so you may not be certain that it's actually working unless it aborts with an error, which would look like
[one of these](http://en.wikipedia.org/wiki/AddressSanitizer#Examples").

You can check that Asan is linked in to your executable using ldd:

<pre>
$ ldd a.out
	linux-vdso.so.1 =>  (0x00007fff749ff000)
	libasan.so.0 => /usr/local/lib64/libasan.so.0 (0x00007f57065f7000)
	libstdc++.so.6 => /usr/local/lib64/libstdc++.so.6 (0x00007f57062ed000)
	libm.so.6 => /lib64/libm.so.6 (0x0000003dacc00000)
	libgcc_s.so.1 => /usr/local/lib64/libgcc_s.so.1 (0x00007f57060bd000)
	libc.so.6 => /lib64/libc.so.6 (0x0000003dad000000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x0000003dad800000)
	libdl.so.2 => /lib64/libdl.so.2 (0x0000003dad400000)
	/lib64/ld-linux-x86-64.so.2 (0x0000003dac800000)
</pre>

You can also up the default verbosity level of Asan to get an idea of what is going on at run-time:

`export ASAN_OPTIONS="verbosity=1:..."`


If you're using `LD_PRELOAD` to inject Asan into an executable that was not built
using Asan, you may see output that looks like the following:

<pre>
==25140== AddressSanitizer: failed to intercept 'memset'
==25140== AddressSanitizer: failed to intercept 'strcat'
==25140== AddressSanitizer: failed to intercept 'strchr'
==25140== AddressSanitizer: failed to intercept 'strcmp'
==25140== AddressSanitizer: failed to intercept 'strcpy'
==25140== AddressSanitizer: failed to intercept 'strlen'
==25140== AddressSanitizer: failed to intercept 'strncmp'
==25140== AddressSanitizer: failed to intercept 'strncpy'
==25140== AddressSanitizer: failed to intercept 'pthread_create'
==25140== AddressSanitizer: libc interceptors initialized
</pre>

Don't worry -- it turns out that is a bogus warning related to running Asan as a shared object.  Unfortunately, the Asan
developers don't seem to want to fix this (http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58680).    

Conclusion
----------

So, how did this all turn out?  Well, it's pretty early in the process, but Asan
has already caught a memory corruption problem that would have been extremely
difficult to track down otherwise.  (Short version is that due to some
unintended name collissions between shared libraries, we were trying to put 10
pounds of bologna in a 5 pound sack.  Or, as one of my colleagues more accurately pointed out, 8 pounds
of bologna in a 4 pund sack :-)

valgrind is still an extremely valuable tool, especially because of its
convenience and versatility; but in certain edge cases Asan can bring things to
the table, like speed and selectivity, that make it the better choice.

Postscript 
-----------

Before closing there are a few more things I want to mention about Asan in
comparison to valgrind:

-   If you look at the processes using Asan with top, etc. you may be a bit
    shocked at first to see they are using 4TB (or more) of memory.  Relax --
    it's not real memory, it's virtual memory (i.e., address space).  The
    algorithm used by Asan to track memory "shadows" actual memory (one bit for
    every byte), so it needs that whole address space.  Actual memory use is
    greater with Asan as well, but not nearly as bad as it appears at first
    glance.  Even so, Asan disables core files by default, at least in 64-bit
    mode.

-   As hoped, Asan is way faster than valgrind, especially in my "worst-case"
    scenario with the JVM, since the only code that's paying the price of
    tracking memory accesses is the code that is deliberately instrumented.
    That also eliminates false positives from the JVM, which is a very good
    thing.

-   As for false positives, the Asan folks apparently don't believe in them,
    because there is no "suppression" mechanism like there is in valgrind.
    Instead, the Asan folks ask that if you find what you think is a false
    positive, you file a bug report with them.  In fact, when Asan finds a
    memory error it immediately aborts -- the rationale being that allowing Asan
    to continue after a memory error would be much more work, and would make
    Asan much slower.  Let's hope they're right about the absence of false
    positives, but even so this "feature" is bound to make the debug cycle
    longer, so there are probably cases where valgrind is a better choice -- at
    least for initial debugging.
    
-   Asan and valgrind have slightly different capabilities, too:

    -   Asan can find stack corruption errors, while valgrind only tracks heap
        allocations.

    -   Both valgrind and Asan can detect memory leaks (although Asan's leak
        checking support is "still experimental" - see
        <http://code.google.com/p/address-sanitizer/wiki/LeakSanitizer>).

    -   valgrind also detects reads of un-initialized memory, which Asan does
        not.

        -   The related [Memory Sanitizer](https://code.google.com/p/memory-sanitizer/wiki/MemorySanitizer)
            tool apparently can do that.  It has an additional restriction that
            the main program must be built with -fpie to enable
            position-independent code, which may make it difficult to use in
            certain cases, e.g. for debugging code hosted in a JVM.

A detailed comparison of Asan, valgrind and other tools can be found [here](<http://code.google.com/p/address-sanitizer/wiki/ComparisonOfMemoryTools>).


Resources
--------------------

<http://en.wikipedia.org/wiki/AddressSanitizer>

http://code.google.com/p/address-sanitizer/

http://clang.llvm.org/docs/AddressSanitizer.html



[^3]: In this paper, I use the term valgrind, but I really mean valgrind with the memcheck tool.  valgrind includes a bunch of other tools as well -- see <http://valgrind.org> for details.

[^2]: As is another tool, the Thread Sanitizer, which detects data races between threads at run-time.  More on that in an upcoming post.

[^5]: Omitting the frame pointer makes another register (ebp) available to the compiler, but since there are already at least a dozen other registers for the compiler to use, this extra register is unlikely to be critical.  The compiler can also omit the code that saves and restores the register, but that's a couple of instructions moving data between registers and L1 cache. 
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clock Sources in Linux]]></title>
    <link href="http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux/"/>
    <updated>2014-02-18T20:07:41-05:00</updated>
    <id>http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux</id>
    <content type="html"><![CDATA[<p>For measuring latency in modern systems, we need to be able to measure intervals
in microseconds at least, and preferably in nanoseconds or better. The good news
is that with relatively modern hardware and software, it is possible to
accurately measure time intervals as small as (some smallish number of)
nanoseconds. But, it's important to understand what you're measuring and what
the different edge cases might be to ensure that your results are accurate.

<!--more-->

TL;DR
-----

The short version is that for best results you should be using:

-   Linux kernel 2.6.18 or above -- this is the first version that includes the
    hrtimers package. Even better is 2.6.32 or above, since this includes
    support for most of the different clock sources.

-   A CPU with a constant, invariant TSC (time-stamp counter). This means that
    the TSC runs at a constant rate across all sockets/cores, regardless of
    frequency changes made to the CPU by power management code. If the CPU
    supports the rdtscp instruction, so much the better.

-   The TSC should be configured as the clock source for the Linux kernel at
    boot time.

-   You should be measuring the interval between two events that happen on the
    same machine (intra-machine timing).

-   For intra-machine timing, your best bet is generally going to be to read the
    TSC directly using assembler. On my test machine it takes about 100ns 
    to read the TSC from software, so that is the limit of this method's accuracy. YMMV, of course, which is why I've included [source code](<https://github.com/btorpey/clocks.git>) that you can use to do your own measurements.
    
The following sections will talk about how clocks work on Linux, how to access
the various clocks from software, and how to measure the overhead of acessing
them.

### Intra-machine vs. Inter-machine Timings

However, before jumping into the details of the above recommendations, I want to
talk a little about the different problems in intra-machine vs. inter-machine
time measurements. Intra-machine timing is the simplest scenario, since it is
generally pretty easy to ensure that you use the same clock source for all your
timing measurements.

The problem with inter-machine timing is that, by definition, you're dealing
with (at least) two different clock sources. (Unless of course you are timing
round-trip intervals -- if that's the case, you're lucky). And the problem with
having two clock sources is described somewhat amusingly by this old chestnut: 

<blockquote>A man with a watch knows what time it is. A man with two watches is never sure.<footer><cite>Segal's Law</cite></footer></blockquote>

For inter-machine timings, you're pretty much stuck with the CLOCK_REALTIME
clock source (the source for gettimeofday), since you presumably need a clock
that is synchronized across the two (or more) machines you are testing. In this
case, the accuracy of your timing measurements will obviously depend on how well
the clock synchronization works, and in all but the best cases you'll be
lucky to get accuracy better than some small number of microseconds.[^1]

We're not going to talk much more about inter-machine timing in this article,
but may get into it another time.

How Linux Keeps Time
--------------------

With that out of the way, let's take a look at how Linux keeps time. It starts
when the system boots up, when Linux gets the current time from the RTC (Real
Time Clock). This is a hardware clock that is powered by a battery so it
continues to run even when the machine is powered off. In most cases it is not
particularly accurate, since it is driven from a cheap crystal oscillator whose
frequency can vary depending on temperature and other factors.[^2] The boot
time retrieved from the RTC is stored in memory in the kernel, and is used as an
offset later by code that derives wall-clock time from the combination of boot
time and the tick count kept by the TSC.

The other thing that happens when the system boots is that the TSC (Time Stamp
Counter) starts running. The TSC is a register counter that is also driven from
a crystal oscillator -- the same oscillator that is used to generate the clock
pulses that drive the CPU(s). As such it runs at the frequency of the CPU, so
for instance a 2GHz clock will tick twice per nanosecond.

There are a number of other clock sources which we'll discuss later, but in most
cases the TSC is the preferred clock source for two reasons: it is very
accurate, and it is very cheap to query its value (since it is simply a
register). But, there are a number of caveats to keep in mind when using the TSC
as a timing source.

-   In older CPU's, each core had its own TSC, so in order to be sure that two
    measurements were accurate relative to each other, it was necessary to pin
    the measuring code to a single core.

-   Also in older CPU's, the TSC would run at the frequency of the CPU itself,
    and if that changed (for instance, if the frequency was dynamically reduced,
    or the CPU stopped completely for power management), the TSC on that CPU
    would also slow down or stop. (It is sometimes possible to work around this
    problem by disabling power management in the BIOS, so all CPU's always run
    at 100%  no more, no less).

Both of these problems are solved in more recent CPUs: a *constant* TSC keeps
all TSC's synchronized across all cores in a system, and an *invariant* (or
*nonstop*) TSC keeps the TSC running at a constant rate regardless of changes in
CPU frequency. To check whether your CPU supports one or both, execute the
following and examine the values output in flags:

<pre>
$ cat /proc/cpuinfo | grep -i tsc
flags : ... tsc  rdtscp constant_tsc nonstop_tsc ...
</pre>

The flags have the following meanings:

tsc
:  The system has a TSC clock.

rdtscp
:  The rdtscp instruction is available.

constant_tsc
:  The TSC is synchronized across all sockets/cores.

nonstop_tsc
:  The TSC is not affected by power management code.

### Other Clock Sources

While the TSC is generally the preferred clock source, given its accuracy and
relatively low overhead, there are other clock sources that can be used:

-   The HPET (High Precision Event Timer) was introduced by Microsoft and Intel
    around 2005. Its precision is approximately 100 ns, so it is less accurate
    than the TSC, which can provide sub-nanosecond accuracy. It is also much
    more expensive to query the HPET than the TSC.

-   The acpi_pm clock source has the advantage that its frequency doesn't change
    based on power-management code, but since it runs at 3.58MHz (one tick every
    279 ns), it is not nearly as accurate as the preceding timers.

-   jiffies signifies that the clock source is actually the same timer used for
    scheduling, and as such its resolution is typically quite poor. (The default
    scheduling interval in most Linux variants is either 1 ms or 10 ms).

To see the clock sources that are available on the system:

<pre>
$ cat /sys/devices/system/clocksource/clocsource0/available_clocksource
tsc hpet acpi_pm
</pre>

And to see which one is being used:

<pre>
$ cat /sys/devices/system/clocksource/clocksource0/current_clocksource
tsc
</pre>

Typically the clock source is set by the kernel automatically at boot time, but
you can force a particular clock source by including the appropriate
parameter(s) on the command line that boots Linux (e.g., in
/boot/grub/grub.conf):

`ro root=/dev/... clocksource=tsc`

You can also change the clock source while the system is running  e.g., to
force use of HPET:

<pre>
$ echo hpet > /sys/devices/system/clocksource/clocksource0/current_clocksource
</pre>

The above discusssion refers to what I will call hardware clocks, although
strictly speaking these clocks are a mixture of hardware and software. At the
bottom of it all there's some kind of hardware device that generates periodic
timing pulses, which are then counted to create the clock. In some cases (e.g.,
the TSC) the counting is done in hardware, while in others (e.g., jiffies) the
counting is done in software.

Wall-Clock Time
---------------

The hardware (or hardware/software hybrid) clocks just discussed all have one
thing in common: they are simply counters, and as such have no direct
relationship to what most of us think of as time, commonly referred to as
wall-clock time.

To derive wall-clock time from these counters requires some fairly intricate
software, at least if the wall-clock time is to be reasonably accurate. What
reasonably accurate means of course depends on how important it is (i.e., how
much money is available) to make sure that wall-clock time is accurate. 

The whole process of synchronizing multiple distributed clocks is hellishly complicated, and we're not going to go into it here. There are many different mechanisms for synchronizing distributed clocks, from the relatively simple (e.g., NTP[^3]) to the not-quite-so-simple (e.g., PTP[^4]), up to specialized proprietary solutions[^5].

The main point is that synchronizing a system's wall-clock time with other
systems requires a way to adjust the clock to keep it in sync with its peers.
There are two ways this can be done:

-   Stepping is the process of making (one or more) discontinuous changes to
    the wall-clock component of the system time. This can cause big jumps in the
    wall-clock time, including backwards jumps, although the time adjustment
    software can often be configured to limit the size of a single change. A
    common example is a system that is configured to initialize its clock at
    boot time from an NTP server.

-   Slewing (sometimes called disciplining) involves actually changing the frequency (or frequency
    multiplier) of the oscillator used to drive a hardware counter like the TSC.
    This can cause the clock to run relatively faster or slower, but it cannot
    jump, and so cannot go backwards.
    


Available Clock Sources
-----------------------

The most common way to get time information in Linux is by calling the
gettimeofday() system call, which returns the current wall-clock time with
microsecond precision (although not necessarily microsecond accuracy). Since
gettimeofday() calls clock_gettime(CLOCK_REALTIME, ), the following discussion
applies to it as well.

Linux also implements the POSIX clock_gettime() family of functions, which let
you query different clock sources, including:

<table id="mytab">
<tbody>
<tr>
  <td>CLOCK_REALTIME </td>
  <td>Represents wall-clock time. Can be both stepped and slewed by time adjustment code (e.g., NTP, PTP).</td>
</tr>
<tr>
  <td>CLOCK_REALTIME_COARSE </td>
  <td>A lower-resolution version of CLOCK_REALTIME.</td>
</tr>
<tr>
  <td>CLOCK_REALTIME_HR  </td>
  <td>A higher-resolution version of CLOCK_REALTIME. 
                        Only available with the real-time kernel.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC </td>
  <td>Represents the interval from an abitrary time. 
                        Can be slewed but not stepped by time adjustment code. 
                        As such, it can only move forward, not backward.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC_COARSE </td>
  <td>A lower-resolution version of CLOCK_MONOTONIC.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC_RAW </td>
  <td>A version of CLOCK_MONOTONIC that can neither be slewed nor stepped by time adjustment code.</td>
</tr>
<tr>
  <td>CLOCK_BOOTTIME</td>
  <td>A version of CLOCK_MONOTONIC that additionally reflects time spent in suspend mode.  Only available in newer (2.6.39+) kernels.</td>
</tr>
</tbody>
</table>
<br>

The availability of the various clocks, as well as their resolution and
accuracy, depends on the hardware as well as the specific Linux implementation.
As part of the [accompanying source code](<https://github.com/btorpey/clocks.git>) for this article I've
included a small test program (clocks.c) that when compiled[^6] and run will
print the relevant information about the clocks on a system. On my test
machine[^7] it shows the following:

<pre>
clocks.c
                    clock	       res (ns)	           secs	          nsecs
             gettimeofday	          1,000	  1,391,886,268	    904,379,000
           CLOCK_REALTIME	              1	  1,391,886,268	    904,393,224
    CLOCK_REALTIME_COARSE	        999,848	  1,391,886,268	    903,142,905
          CLOCK_MONOTONIC	              1	        136,612	    254,536,227
      CLOCK_MONOTONIC_RAW	    870,001,632	        136,612	    381,306,122
   CLOCK_MONOTONIC_COARSE	        999,848	        136,612	    253,271,977
</pre>

Note that it's important to pay attention to what clock_getres() returns -- a particular clock source can (and does, as can be seen above with the COARSE clocks) sometimes return what may look like higher-precision values, but any digits beyond its actual precision are likely to be garbage.  (The exception is gettimeofday -- since it returns a timeval, which is denominated in micros, the lower-order digits are all zeros).

Also, the value returned from clock_getres() for CLOCK_MONOTONIC_RAW is clearly garbage, although I've seen similar results on several machines.

Finally, note that the resolution listed for CLOCK_REALTIME is close to, but not
quite, 1 million -- this is an artifact of the fact that the oscillator cannot
generate a frequency of exactly 1000 Hz -- it's actually 1000.15 Hz.

Getting Clock Values in Software
--------------------------------

Next up is a brief discussion of how to read these different clock values from
software.

### Assembler

In assembler language, the RDTSC instruction returns the value of the TSC
directly in registers edx:eax. However, since modern CPU's support out-of-order
execution, it has been common practice to insert a serializing instruction (such
as CPUID) prior to the RDTSC instruction in order to ensure that the execution
of RDTSC is not reordered by the processor.

More recent CPU's include the RDTSCP instruction, which does any necessary
serialization itself. This avoids the overhead of the CPUID instruction, which
can be considerable (and variable). If your CPU supports RDTSCP, use that instead of the
CPUID/RDTSC combination.

### C/C++

Obviously, the RDTSC instruction can be called directly from C or C++, using
whatever mechanism your compiler provides for accessing assembler language, or
by calling an assembler stub that is linked with the C/C++ program. (An example
can be found at [Agner Fog's excellent website](<http://agner.org/optimize/#asmlib>)).

Calling gettimeofday() or clock_gettime() is pretty straightforward -- see the
accompanying [clocks.c source file](<https://github.com/btorpey/clocks/blob/master/clocks.c>) for examples.



### Java

Java has only two methods that are relevant to this discussion:

-   System.currentTimeMillis() returns the current wall-clock time as the number
    of milliseconds since the epoch. It calls gettimeofday(), which in turn
    calls clock_gettime(CLOCK_REALTIME, ...).

-   System.nanoTime returns the number of nanoseconds since some unspecified
    starting point. Depending on the capabilities of the system, it either calls
    gettimeofday(), or clock_gettime(CLOCK_MONOTONIC, ).

The bad news is that if you need clock values other than the above in Java,
you're going to need to roll your own, e.g. by calling into C via JNI. The good
news is that doing so is not much more expensive than calling nanoTime (at least in my tests).

### Overhead of Clock Queries

The Heisenberg Uncertainty Principle says, in a nutshell, that the act of
observing a phenomenom changes it. A similar issue exists with getting
timestamps for latency measurement, since it takes a finite (and sometimes
variable) amount of time to read any clock source.  In other words, just because the TSC on a 2GHz machine ticks twice per nanosecond doesn't mean we can measure intervals of a nanosecond -- we also need to account for the time it takes to read the TSC from software.

So, how expensive is it to perform these different clock queries? Included is some [sample code](<https://github.com/btorpey/clocks.git>) that you can
use to measure the time it takes to query various clock sources, from both C++
and Java (using JNI to call C code).

Both the C++ and Java versions take the same approach: call the particular clock
function in a tight loop, and store the result. We do this a large number of
times, and hang on to the results from the final iteration. This has the effect
of allowing Java to do any jitting it needs to, and for both the C++ and Java
versions to help ensure that code and data is in the processor's cache memory.

The results of running the test on my test machine are:

<pre>
ClockBench.cpp
                   Method       samples     min     max     avg  median   stdev
           CLOCK_REALTIME       200       57.00   81.00   58.24   69.00    2.99
    CLOCK_REALTIME_COARSE       200        0.00    0.00    0.00    0.00    0.00
          CLOCK_MONOTONIC       200       57.00   84.00   57.52   70.50    2.82
      CLOCK_MONOTONIC_RAW       200      652.00 1104.00  697.26  878.00   60.81
   CLOCK_MONOTONIC_COARSE       200        0.00    0.00    0.00    0.00    0.00
              cpuid+rdtsc       200       96.00  100.00   97.74   98.00    1.57
                    rdtsc       200       27.00   28.00   27.07   27.50    0.26
Using CPU frequency = 2.660000

ClockBench.java
                   Method       samples     min     max     avg  median   stdev
          System.nanoTime       200      111.00  115.00  113.10  113.00    1.49
           CLOCK_REALTIME       200      108.00  114.00  110.16  111.00    1.56
              cpuid+rdtsc       200      153.00  160.00  154.83  156.50    1.64
                    rdtsc       200       75.00   79.00   77.43   77.00    1.35
Using CPU frequency = 2.660000
</pre>


A few things to note about these results:

-   Both of the COARSE clocks show a latency of zero for getting the clock
    value. This tells us that the time it takes to
    get the clock value is less than the resolution of the clock. (Our previous
    test showed a resolution of 1ms for the COARSE clocks).

-   For some reason, the CLOCK_MONOTONIC_RAW clock is very expensive to query. I
    can't explain this --  you would think that its lack of adjustment would make
    it faster, not slower. This is unfortunate, as otherwise it would be an
    excellent choice for intra-machine timing.

-   As you might expect, the combination of cpuid and rdtsc is slower than
    rdtscp, which is slower than rdtsc alone. In general, this would
    suggest that rdtscp should be preferred if available, with a fallback to
    cpuid+rdtsc if not. (While rdtsc alone is the fastest, the fact that it can
    be inaccurate as a result of out-of-order execution means it is only useful
    for timing relatively long operations where that inaccuracy is not
    significant -- but those are precisely the scenarios where its speed is less
    important).

-   Also as expected, the Java versions are slightly slower than the C++
    versions, presumably due to the overhead of going through JNI.

Conclusion
----------

I thought this would be a very brief and somewhat trivial research project. In
fact, it turned out to be far more complicated (and less well-documented) than I
expected. I guess I should have known: everything related to time and computers
turns out to be a major pain in the neck!

Anyway, I hope this proves helpful. (I know I would have been very happy to have
had this when I started looking into clock sources).  

As always, please feel free to [contact me](<mailto:wallstprog@gmail.com>)
directly with comments, suggestions, corrections, etc.

Additional Resources
--------------------

Following are the main anchor points that I kept coming back to you as I
researched this article.

<http://elinux.org/Kernel_Timer_Systems>

<http://elinux.org/High_Resolution_Timers>

<http://juliusdavies.ca/posix_clocks/clock_realtime_linux_faq.html>

<http://en.wikipedia.org/wiki/Time_Stamp_Counter>

<http://stackoverflow.com/questions/10921210/cpu-tsc-fetch-operation-especially-in-multicore-multi-processor-environment>

<http://www.citihub.com/requesting-timestamp-in-applications/>

<http://www.intel.com/content/www/us/en/intelligent-systems/embedded-systems-training/ia-32-ia-64-benchmark-code-execution-paper.html>



[^1]:  The best case being hardware on each machine with a CSAC (chip-scale atomic clock) or OCXO (oven-controlled crystal oscillator). These can be a bit pricey, however.

[^2]:  The accuracy of a typical RTC in a PC-type computer is rated at +/- 20ppm, so it can gain or lose 20 us each second. This turns out to be approximately one minute per month, which may be OK for a cheap digital watch, but for a computer is not too good. For more information, see <http://www.maximintegrated.com/app-notes/index.mvp/id/58>.

[^3]: Network Time Protocol, RFC 1305 (<https://tools.ietf.org/html/rfc1305>)

[^4]: Precision Time Protocol, IEEE 1588 (<http://www.nist.gov/el/isd/ieee/ieee1588.cfm>)

[^5]: From companies like Symmetricon, Corvil, TS Associates and others.

[^6]:  Note that the program must be compiled, as well as run, on the target system -- it uses the presence or absence of pre-processor symbols to determine whether a particular clock source is available.

[^7]: CentOS 6.5 running on a Dell 490 with dual Xeon 5150's at 2.6 GHz.

</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[You may ask yourself - "How did I get here?"]]></title>
    <link href="http://btorpey.github.io/blog/2014/02/13/how-did-i-get-here/"/>
    <updated>2014-02-13T18:02:27-05:00</updated>
    <id>http://btorpey.github.io/blog/2014/02/13/how-did-i-get-here</id>
    <content type="html"><![CDATA[<p>In addition to being a great line from David Byrne and Talking Heads (from "Life
During Wartime"), this is also a question I often ask myself when
looking at log files. Today's tip is worth the price of the whole blog (i.e.,
free), but I predict that you'll be glad you know it.

<!--more-->

It's pretty common to pipe the output of a command, or string of commands, to a
file to have a record of what happened when executing the command, something
like this:

`big_gnarly_command_line_with_options 2>&1 | tee logfile.out`

That works great for capturing the *output* of the command, but what about the
big_gnarly_command_line_with_options itself?

Try this instead:

` bash -x -c "big_gnarly_command_line_with_options" 2>&1 | tee logfile.out`


Now, your output file will look like this:

`+ big_gnarly_command_line_with_options`<br>
`... output of big_gnarly_command_line_with_options ...`

If your gnarly command is actually several gnarly commands, enclose the whole
gnarly list in parentheses and separate with semicolons (or &&), like so:

` bash -x -c "(big_gnarly_command_line_with_options_1;
big_gnarly_command_line_with_options_2)" 2>&1 | tee logfile.out`

Normal quoting rules apply:

-   If you enclose the command(s) in double-quotes ("), variable substitution
    will be done on the command line

-   If you need to include a double-quote within double-quotes, you need to
    escape it (with the backslash (\\) character)

-   If you enclose the command line(s) in single quotes ('), no variable
    substitution is done

-   There is no way to include a single-quote within single-quotes, but there is
    a trick that gives a similar effect, that you can read about here (<http://stackoverflow.com/a/1250279/203044>).

Now you'll never need to ask yourself "How did I get here"?

  
  

</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FIXing less]]></title>
    <link href="http://btorpey.github.io/blog/2014/02/10/fixing-less/"/>
    <updated>2014-02-10T08:37:11-05:00</updated>
    <id>http://btorpey.github.io/blog/2014/02/10/fixing-less</id>
    <content type="html"><![CDATA[<p>Here's a handy tip for those who (like me) spend a fair amount of time staring
at FIX logs.

<!--more-->

FIX may be the protocol that everybody loves to hate, but it doesn't look like it's
going anywhere, so I guess we all just need to get over it and learn to live with it.

One of the things that is hard to live with, though -- at least for me -- is the
visual cacophony that results when browsing FIX logs with less.
<img class="center" src="/images/less-before.png"> 

It turns out that it's possible to control how less displays the x'01'
delimiters to make this chore a little easier on the eyes.  In my case, I
use the following in my .bash_profile:

`export LESSBINFMT="*u%x"`

This dials down the visual clutter to a level that I find much easier to deal
with.
<img class="center" src="/images/less-after.png"> 


(Note that the man page for less mentions that it's possible to display the hex codes in square brackets, but I have not found that to work on any of the systems where I've tried it -- YMMV).
</p>
]]></content>
  </entry>
  
</feed>
