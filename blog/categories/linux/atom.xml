<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Confessions of a Wall Street Programmer]]></title>
  <link href="http://btorpey.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://btorpey.github.io/"/>
  <updated>2014-06-01T11:58:20-04:00</updated>
  <id>http://btorpey.github.io/</id>
  <author>
    <name><![CDATA[Bill Torpey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Where Am I?]]></title>
    <link href="http://btorpey.github.io/blog/2014/05/29/where-am-i/"/>
    <updated>2014-05-29T09:09:31-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/05/29/where-am-i</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/gilligans-island-tv-show.jpg" width="320" height="240"> 

From *Robinson Crusoe* to *Gilligan's Island* to *Lost*, tales of being
stranded on a desert island seem to resonate with people in a special way. Some
of that likely has to do with the exotic locales, and the practical challenges
of getting water, food and shelter.

But an even more basic part is the unanswered question: "Where am I?" that makes
things so -- well, *mysterious*.

Shell scripting can be pretty mysterious too at times, but in this installment
we'll learn how to answer that basic question of "Where am I?" to make shell
scripting a little less mysterious.

<!--more-->

One of the tenets of the Unix way is brevity, and one consequence of that is
that well-behaved programs should be able to find whatever other resources they
need without having to be told where they are. Windows attempts to
solve this problem with the (gack!) registry, but Unix tends to use a simpler
approach: needed resources are placed either in well-known locations (e.g., /etc
for system programs), or where they can be found relative to the location of the
program itself.

Another attribute of a well-behaved Unix program is that it
should be able to run from any location, whether it's invoked with a full path,
or found via the PATH variable.

So, how do we reconcile those two requirements? And specifically, how do we do
that in shell scripts? Since -- regardless of what your "main" language is, if
you're programming in Unix/Linux, you're probably also writing a boatload of
shell scripts too.

It turns out that, at least in bash, there is a
simple but non-obvious way to do get the location of the script file itself,
which goes something like this:

    SCRIPT_DIR=$(cd $(dirname ${BASH_SOURCE}) && /bin/pwd) 

Let's follow this through and see how it works:

-   The `$( ... )` construct invokes a sub-shell. This is handy since it
allows us to change the environment of the sub-shell (e.g., current directory)
without affecting the current environment.

-   `$BASH_SOURCE` is a builtin variable that gives us the path to the shell
script itself. For instance, if we invoke a script with `./scriptname.sh`,
then that's what will end up in `${BASH_SOURCE}`.

-   To get the full path then we extract just the path part with `dirname`, again
in a sub-shell.

-   We then `cd` into that directory, and if successful get the full pathname
with `/bin/pwd`.
    -   Note that we use `/bin/pwd` to get the path. This version resolves any
    symbolic links to return the actual physical path. There is also a `pwd`
    built-in to bash, but that one does not expand symbolic links by default.
<br>
-   Finally, the result is assigned to SCRIPT_DIR.

We now have the full path of the script file itself, and can use that to locate
any other resources needed by the script. For a real-world example, you can
check out the [these scripts](<https://github.com/btorpey/latency-utils.git>) from
 [my earlier post on visualizing latency](</blog/2014/05/16/visualizing-latency/>).


</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visualizing Latency]]></title>
    <link href="http://btorpey.github.io/blog/2014/05/16/visualizing-latency/"/>
    <updated>2014-05-16T08:42:58-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/05/16/visualizing-latency</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/ping.png" width="320" height="240"> 

I'm a visual thinker (I think I may have [mentioned that before](http://btorpey.github.io/blog/2014/04/29/a-picture-is-worth-1k-words/) ),
so when I'm analyzing performance, latency, etc. I find it really helpful to be
able to visualize what is going on on the machine. 

As a result, I had gotten
reasonably good at using Excel to produce charts, which sometimes helped to correlate observed
behaviors like latency spikes with other events on the machine.

For a bunch of reasons I wanted to move away from Excel, though, and find
another tool that would give me the same or better functionality.

<!--more-->

For one thing, a little over a year ago I switched to a Mac as my main machine
after years of using Windows. There was a certain amount of adjustment, but for
the most part it's been smooth sailing. More than that, I was actually able to
recapture some of the fun and excitement I remember from my first Apple (an
Apple ][).

I also wanted something that would run on both the Mac and Linux, where I do
most of my testing. Last but not least, I wanted something that would be
scriptable so I could easily produce consistent charts for multiple test runs.

I looked briefly at R, but ditched it when it used up all the 8GB in my laptop,
plus the entire hard disk as swap, for a single dataset of 100,000 points.
Probably my bad, but I didn't have the patience to figure out what I might be
doing wrong.

At that point I turned to venerable (some would say crusty) gnuplot. It's a bit
long in the tooth, but I just wanted to plot latency over time, so how hard
could that be? Well, I guess it's pretty easy if you already know how, but
starting from scratch is another story.

Which brings me to my rant of the day, directed at techies in general, and to the
(us?) Linux/Unix techies in particular.

Short version: I don't want to learn gnuplot. I don't even want to *have
learned* gnuplot -- even if I could do that by just taking a pill. What I want
is to be able to produce decent-looking charts *without* knowing *anything*
about gnuplot.

To be fair, the gnuplot docs did have some examples -- more anyway than you
would find in a typical man page, although that's admittedly a low bar. And
while my google-fu is usually pretty good, I just couldn't find anything on the
intertubes that would work for me, so I had to learn *just a little* gnuplot.

> When all else fails, read the instructions.

It turns out that gnuplot works pretty well, and will probably work even better
once I learn (sigh) how to use it better.

But you don't have to learn diddly if you don't want to. [Here is the first](<https://github.com/btorpey/latency-utils.git>) in
what will hopefully be a series of recipes that you can use with little or no
modification.  Once you've downloaded the repo, enter the following at the command prompt:

`./tsd.sh ping.csv x11`

Which should result in something like this:

<img class="center" src="/images/gnuplot.png"> 

It's primitive, but that very primitiveness has its own appeal, especially for
those of us for whom "UI" means bash, vi or emacs.

A couple of points about the gnuplot command files:

-   Sometimes you care about the actual time that an event took place, so you
can correlate it with some other event; sometimes you don't. Accordingly, I've
created two different files: one which displays actual time (ts.gp), the other
which calculates and displays deltaT (tsd.gp).

-   I've been programming in C (and later C++) for many years, but I don't think
I've ever purposely used the comma operator before. Well, expressions in gnuplot
follow C language rules for operators, precedence, etc. and that comma operator
turns out to be handy -- in this case it lets us 
update the origin in the same expression that calculates deltaT.
(The return value of the comma
operator is the right-hand expression).

-- (Note that the above requires something like gnuplot 4.6)


-   I've left the default terminal in the gnuplot command files, but you can 
specify a different one on the command line.  To get a list of terminals supported 
in your version:
`gnuplot -e "set terminal"`.

Comments, suggestions, pull requests, etc. welcome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Picture is Worth 1K Words]]></title>
    <link href="http://btorpey.github.io/blog/2014/04/29/a-picture-is-worth-1k-words/"/>
    <updated>2014-04-29T19:23:07-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/04/29/a-picture-is-worth-1k-words</id>
    <content type="html"><![CDATA[<p>You know those mutiple-choice tests that put you in one of four quadrants based
on your answers to a bunch of seemingly irrelevant questions? We've all taken
them, and if you're like me they're kind of like reading your horoscope -- it
all seems so right and true when you're reading it, but you wonder if it would
still seem just as right and true if the horoscopes got jumbled at random?

Well, I took one of these tests a while back that actually told me something about myself -- it was the "Learning-Style Inventory"
test, and what it said about me is that I'm waaaayyy over at the end of the
scale when it comes to visual thinking. That gave me an insight into the way my
brain works that I've found really helpful ever since. So, this next bit was right up my alley,
but I'm guessing you'll like it too.

[<img class="right" src="http://overbyte.com.au/wp-content/uploads/2012/01/InteractiveMemAccess-620x424.png" width="320" height="240">](<http://www.overbyte.com.au/misc/Lesson3/CacheFun.html>)

We read a lot lately about NUMA architecture and how it presents a fundamental
change in the way we approach writing efficient code: it's no longer about the
CPU, it's all about RAM. We all nod and say "Sure, I get that!"  Well, I thought
I got it too, but until I saw [this web page](<http://www.overbyte.com.au/misc/Lesson3/CacheFun.html>), 
I really didn't. 

See the full discussion at <http://overbyte.com.au/index.php/overbyte-blog/entry/optimisation-lesson-3-the-memory-bottleneck>.
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using clang's Address Sanitizer (without clang)]]></title>
    <link href="http://btorpey.github.io/blog/2014/03/27/using-clangs-address-sanitizer/"/>
    <updated>2014-03-27T08:30:22-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/03/27/using-clangs-address-sanitizer</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://planet.clang.org/images/DragonSmall.png"> 

Valgrind has been an indispensable tool for C/C++ programmers for a long
time, and I've used it quite happily -- it's a tremendous tool for doing dynamic
analysis of program behavior at run time. valgrind[^3] can detect reads of
uninitialized memory, heap buffer overruns, memory leaks, and other errors that
can be difficult or impossible to find by eyeballing the code, or by static
analysis tools.  But that comes with a price, which in some cases can be quite steep, and some new
tools promise to provide some or all of the functionality valgrind provides without the drawbacks.

<!--more-->

For one thing, valgrind can
be *extremely* slow.  That is an unavoidable side-effect of one of valgrind's
strengths, which is that it doesn't require that the program under test be
instrumented beforehand -- it can analyze any executable (including shared
objects) "right out of the box".  That works because valgrind effectively
emulates the hardware the program runs on, but that leads to a potential
problem: valgrind instruments *all* the code, including shared objects --and
that includes third-party code (e.g., libraries, etc.) that you may not have any
control over.

In my case, that ended up being a real problem.  The main reason
being that a significant portion of the application I work with is hosted in a
JVM (because it runs in-proc to a Java-based FIX engine, using a thin JNI
layer).  The valgrind folks say that the slowdown using their tool can be up to
20x, but it seemed like more, because the entire JVM was being emulated.

And, because valgrind emulates *everything*, it also detects and reports
problems in the JVM itself.  Well, it turns out that the JVM plays a lot of
tricks that valgrind doesn't like, and the result is a flood of complaints that
overwhelm any potential issues in the application itself.

So, I was very interested in learning about a similar technology that promised
to address some of these problems.  Address Sanitizer (Asan from here on) was
originally developed as part of the clang project, and largely by folks at Google.
They took a different approach: while valgrind emulates the machine at run-time, Asan works by instrumenting
the code at compile-time.

That helps to solve the two big problems that I was having with valgrind: its
slowness, and the difficulty of excluding third-party libraries from the
analysis.

Asan with clang
---------------

Since I was already building the application using clang for its excellent
diagnostics and static analysis features, I thought it would be relatively
straightforward to introduce the Asan feature into the build.  Turns out there
is a bump in that road: clang's version of Asan is supplied only as a
static library that is linked into the main executable.  And while it should be
possible to re-jigger things to make it work as a shared library, that would
turn into a bit of science project.  That, and the fact that the wiki page discussing it
(http://code.google.com/p/address-sanitizer/wiki/AsanAsDso) didn't sound
particularly encouraging ("however the devil is in the detail" -- uhh, thanks, no).

Rats!  However, the wiki page
did mention that there was a version of Asan that worked with gcc, and that
version apparently did support deployment as a shared object.  So, I decided to give that a try...

Asan with gcc
-------------

It turns out that the gcc developers haven't been sitting still -- in
fact, it looks like there is a bit of a healthy rivalry between the clang and gcc
folks, and that's a good thing for you and me.  Starting with version 4.8 of the
gcc collection, Asan is available with gcc as well.[^2]

Getting the latest gcc version (4.8.2 as of this writing), building and
installing it was relatively straight-forward.  By default, the source build
installs into /usr/local, so it can co-exist nicely with the native gcc for the
platform (in the case of Red Hat/CentOS 6.5, that is the relatively ancient gcc
4.4 branch).

Building with Asan
-------------
Including support for Asan in your build is pretty simple -- just include the `-fsanitize=address`
flag in both the compile and link step.  (Note that this means you need to invoke the linker via the compiler
driver, rather than directly.  In practice, this means that the executable you specify for the link step should be 
g++ (or gcc), not ld).  

While not strictly required, it's also a very good idea to include the `-fno-omit-frame-pointer` flag
in the compile step.  This will prevent the compiler from optimizing away the frame pointer (ebp) register.  While
disabling any optimization might seem like a bad idea, in this case the performance benefit is likely minimal at best[^5], but the 
inability to get accurate stack frames is a show-stopper.

Running with Asan
-------------
If you're checking an executable that you build yourself, the prior steps are all you need -- libasan.so will get linked
into your executable by virtue of the `-fsanitize=address` flag.

In my case, though, the goal was to be able to instrument code running in the JVM.  In this case, I had to force libasan.so
into the executable at runtime using `LD_PRELOAD`, like so:

`LD_PRELOAD=/usr/local/lib64/libasan.so.0 java ...`

And that's it!

Tailoring Asan
---------------

There are a bunch of options available to tailor the way Asan works: at compile-time you can supply a "blacklist" of functions that
Asan should NOT instrument, and at run-time you can further customize Asan using the `ASAN_OPTIONS` environment variable, which
is discussed [here](<http://code.google.com/p/address-sanitizer/wiki/Flags>).
 
By default, Asan is silent, so you may not be certain that it's actually working unless it aborts with an error, which would look like
[one of these](http://en.wikipedia.org/wiki/AddressSanitizer#Examples").

You can check that Asan is linked in to your executable using ldd:

<pre>
$ ldd a.out
	linux-vdso.so.1 =>  (0x00007fff749ff000)
	libasan.so.0 => /usr/local/lib64/libasan.so.0 (0x00007f57065f7000)
	libstdc++.so.6 => /usr/local/lib64/libstdc++.so.6 (0x00007f57062ed000)
	libm.so.6 => /lib64/libm.so.6 (0x0000003dacc00000)
	libgcc_s.so.1 => /usr/local/lib64/libgcc_s.so.1 (0x00007f57060bd000)
	libc.so.6 => /lib64/libc.so.6 (0x0000003dad000000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x0000003dad800000)
	libdl.so.2 => /lib64/libdl.so.2 (0x0000003dad400000)
	/lib64/ld-linux-x86-64.so.2 (0x0000003dac800000)
</pre>

You can also up the default verbosity level of Asan to get an idea of what is going on at run-time:

`export ASAN_OPTIONS="verbosity=1:..."`


If you're using `LD_PRELOAD` to inject Asan into an executable that was not built
using Asan, you may see output that looks like the following:

<pre>
==25140== AddressSanitizer: failed to intercept 'memset'
==25140== AddressSanitizer: failed to intercept 'strcat'
==25140== AddressSanitizer: failed to intercept 'strchr'
==25140== AddressSanitizer: failed to intercept 'strcmp'
==25140== AddressSanitizer: failed to intercept 'strcpy'
==25140== AddressSanitizer: failed to intercept 'strlen'
==25140== AddressSanitizer: failed to intercept 'strncmp'
==25140== AddressSanitizer: failed to intercept 'strncpy'
==25140== AddressSanitizer: failed to intercept 'pthread_create'
==25140== AddressSanitizer: libc interceptors initialized
</pre>

Don't worry -- it turns out that is a bogus warning related to running Asan as a shared object.  Unfortunately, the Asan
developers don't seem to want to fix this (http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58680).    

Conclusion
----------

So, how did this all turn out?  Well, it's pretty early in the process, but Asan
has already caught a memory corruption problem that would have been extremely
difficult to track down otherwise.  (Short version is that due to some
unintended name collissions between shared libraries, we were trying to put 10
pounds of bologna in a 5 pound sack.  Or, as one of my colleagues more accurately pointed out, 8 pounds
of bologna in a 4 pund sack :-)

valgrind is still an extremely valuable tool, especially because of its
convenience and versatility; but in certain edge cases Asan can bring things to
the table, like speed and selectivity, that make it the better choice.

Postscript 
-----------

Before closing there are a few more things I want to mention about Asan in
comparison to valgrind:

-   If you look at the processes using Asan with top, etc. you may be a bit
    shocked at first to see they are using 4TB (or more) of memory.  Relax --
    it's not real memory, it's virtual memory (i.e., address space).  The
    algorithm used by Asan to track memory "shadows" actual memory (one bit for
    every byte), so it needs that whole address space.  Actual memory use is
    greater with Asan as well, but not nearly as bad as it appears at first
    glance.  Even so, Asan disables core files by default, at least in 64-bit
    mode.

-   As hoped, Asan is way faster than valgrind, especially in my "worst-case"
    scenario with the JVM, since the only code that's paying the price of
    tracking memory accesses is the code that is deliberately instrumented.
    That also eliminates false positives from the JVM, which is a very good
    thing.

-   As for false positives, the Asan folks apparently don't believe in them,
    because there is no "suppression" mechanism like there is in valgrind.
    Instead, the Asan folks ask that if you find what you think is a false
    positive, you file a bug report with them.  In fact, when Asan finds a
    memory error it immediately aborts -- the rationale being that allowing Asan
    to continue after a memory error would be much more work, and would make
    Asan much slower.  Let's hope they're right about the absence of false
    positives, but even so this "feature" is bound to make the debug cycle
    longer, so there are probably cases where valgrind is a better choice -- at
    least for initial debugging.
    
-   Asan and valgrind have slightly different capabilities, too:

    -   Asan can find stack corruption errors, while valgrind only tracks heap
        allocations.

    -   Both valgrind and Asan can detect memory leaks (although Asan's leak
        checking support is "still experimental" - see
        <http://code.google.com/p/address-sanitizer/wiki/LeakSanitizer>).

    -   valgrind also detects reads of un-initialized memory, which Asan does
        not.

        -   The related [Memory Sanitizer](https://code.google.com/p/memory-sanitizer/wiki/MemorySanitizer)
            tool apparently can do that.  It has an additional restriction that
            the main program must be built with -fpie to enable
            position-independent code, which may make it difficult to use in
            certain cases, e.g. for debugging code hosted in a JVM.

A detailed comparison of Asan, valgrind and other tools can be found [here](<http://code.google.com/p/address-sanitizer/wiki/ComparisonOfMemoryTools>).


Resources
--------------------

<http://en.wikipedia.org/wiki/AddressSanitizer>

http://code.google.com/p/address-sanitizer/

http://clang.llvm.org/docs/AddressSanitizer.html



[^3]: In this paper, I use the term valgrind, but I really mean valgrind with the memcheck tool.  valgrind includes a bunch of other tools as well -- see <http://valgrind.org> for details.

[^2]: As is another tool, the Thread Sanitizer, which detects data races between threads at run-time.  More on that in an upcoming post.

[^5]: Omitting the frame pointer makes another register (ebp) available to the compiler, but since there are already at least a dozen other registers for the compiler to use, this extra register is unlikely to be critical.  The compiler can also omit the code that saves and restores the register, but that's a couple of instructions moving data between registers and L1 cache. 
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Measuring Latency in Linux]]></title>
    <link href="http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux/"/>
    <updated>2014-02-18T20:07:41-05:00</updated>
    <id>http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux</id>
    <content type="html"><![CDATA[<p>
<table>
    <tr>
        <td>
For measuring latency in modern systems, we need to be able to measure intervals
in microseconds at least, and preferably in nanoseconds or better. The good news
is that with relatively modern hardware and software, it is possible to
accurately measure time intervals as small as (some smallish number of)
nanoseconds. 

But, it's important to understand what you're measuring and what
the different edge cases might be to ensure that your results are accurate.          
        </td>
        <td>
<blockquote>A man with a watch knows what time it is. A man with two watches is never sure.</blockquote>
        
        </td>
    </tr>
</table>    


<!--more-->

TL;DR
-----

The short version is that for best results you should be using:

-   Linux kernel 2.6.18 or above -- this is the first version that includes the
    hrtimers package. Even better is 2.6.32 or above, since this includes
    support for most of the different clock sources.

-   A CPU with a constant, invariant TSC (time-stamp counter). This means that
    the TSC runs at a constant rate across all sockets/cores, regardless of
    frequency changes made to the CPU by power management code. If the CPU
    supports the rdtscp instruction, so much the better.

-   The TSC should be configured as the clock source for the Linux kernel at
    boot time.

-   You should be measuring the interval between two events that happen on the
    same machine (intra-machine timing).

-   For intra-machine timing, your best bet is generally going to be to read the
    TSC directly using assembler. On my test machine it takes about 100ns 
    to read the TSC from software, so that is the limit of this method's accuracy. YMMV, of course, which is why I've included [source code](<https://github.com/btorpey/clocks.git>) that you can use to do your own measurements.
    - Note that the 100ns mentioned above is largely due to the fact that my Linux box
    doesn't support the RDTSCP instruction, so to get reasonably accurate timings it's also necessary
    to issue a CPUID instruction prior to RDTSC to serialize its execution. On another machine that supports
    the RDTSCP instruction (a recent MacBook Air), overhead is down around 14ns.  More on that 
    <a href="#rdtscp">later...</a>
    
The following sections will talk about how clocks work on Linux, how to access
the various clocks from software, and how to measure the overhead of acessing
them.

### Intra-machine vs. Inter-machine Timings

However, before jumping into the details of the above recommendations, I want to
talk a little about the different problems in intra-machine vs. inter-machine
time measurements. Intra-machine timing is the simplest scenario, since it is
generally pretty easy to ensure that you use the same clock source for all your
timing measurements.

The problem with inter-machine timing is that, by definition, you're dealing
with (at least) two different clock sources. (Unless of course you are timing
round-trip intervals -- if that's the case, you're lucky). And the problem with
having two clock sources is described somewhat amusingly by the old chestnut that leads off this article: 

<blockquote>A man with a watch knows what time it is. A man with two watches is never sure.<footer><cite>Segal's Law</cite></footer></blockquote>

For inter-machine timings, you're pretty much stuck with the CLOCK_REALTIME
clock source (the source for gettimeofday), since you presumably need a clock
that is synchronized across the two (or more) machines you are testing. In this
case, the accuracy of your timing measurements will obviously depend on how well
the clock synchronization works, and in all but the best cases you'll be
lucky to get accuracy better than some small number of microseconds.[^1]

We're not going to talk much more about inter-machine timing in this article,
but may get into it another time.

How Linux Keeps Time
--------------------

With that out of the way, let's take a look at how Linux keeps time. It starts
when the system boots up, when Linux gets the current time from the RTC (Real
Time Clock). This is a hardware clock that is powered by a battery so it
continues to run even when the machine is powered off. In most cases it is not
particularly accurate, since it is driven from a cheap crystal oscillator whose
frequency can vary depending on temperature and other factors.[^2] The boot
time retrieved from the RTC is stored in memory in the kernel, and is used as an
offset later by code that derives wall-clock time from the combination of boot
time and the tick count kept by the TSC.

The other thing that happens when the system boots is that the TSC (Time Stamp
Counter) starts running. The TSC is a register counter that is also driven from
a crystal oscillator -- the same oscillator that is used to generate the clock
pulses that drive the CPU(s). As such it runs at the frequency of the CPU, so
for instance a 2GHz clock will tick twice per nanosecond.

There are a number of other clock sources which we'll discuss later, but in most
cases the TSC is the preferred clock source for two reasons: it is very
accurate, and it is very cheap to query its value (since it is simply a
register). But, there are a number of caveats to keep in mind when using the TSC
as a timing source.

-   In older CPU's, each core had its own TSC, so in order to be sure that two
    measurements were accurate relative to each other, it was necessary to pin
    the measuring code to a single core.

-   Also in older CPU's, the TSC would run at the frequency of the CPU itself,
    and if that changed (for instance, if the frequency was dynamically reduced,
    or the CPU stopped completely for power management), the TSC on that CPU
    would also slow down or stop. (It is sometimes possible to work around this
    problem by disabling power management in the BIOS, so all CPU's always run
    at 100%  no more, no less).

Both of these problems are solved in more recent CPUs: a *constant* TSC keeps
all TSC's synchronized across all cores in a system, and an *invariant* (or
*nonstop*) TSC keeps the TSC running at a constant rate regardless of changes in
CPU frequency. To check whether your CPU supports one or both, execute the
following and examine the values output in flags:

<pre>
$ cat /proc/cpuinfo | grep -i tsc
flags : ... tsc  rdtscp constant_tsc nonstop_tsc ...
</pre>

The flags have the following meanings:

tsc
:  The system has a TSC clock.

rdtscp
:  The rdtscp instruction is available.

constant_tsc
:  The TSC is synchronized across all sockets/cores.

nonstop_tsc
:  The TSC is not affected by power management code.

### Other Clock Sources

While the TSC is generally the preferred clock source, given its accuracy and
relatively low overhead, there are other clock sources that can be used:

-   The HPET (High Precision Event Timer) was introduced by Microsoft and Intel
    around 2005. Its precision is approximately 100 ns, so it is less accurate
    than the TSC, which can provide sub-nanosecond accuracy. It is also much
    more expensive to query the HPET than the TSC.

-   The acpi_pm clock source has the advantage that its frequency doesn't change
    based on power-management code, but since it runs at 3.58MHz (one tick every
    279 ns), it is not nearly as accurate as the preceding timers.

-   jiffies signifies that the clock source is actually the same timer used for
    scheduling, and as such its resolution is typically quite poor. (The default
    scheduling interval in most Linux variants is either 1 ms or 10 ms).

To see the clock sources that are available on the system:

<pre>
$ cat /sys/devices/system/clocksource/clocksource0/available_clocksource
tsc hpet acpi_pm
</pre>

And to see which one is being used:

<pre>
$ cat /sys/devices/system/clocksource/clocksource0/current_clocksource
tsc
</pre>

Typically the clock source is set by the kernel automatically at boot time, but
you can force a particular clock source by including the appropriate
parameter(s) on the command line that boots Linux (e.g., in
/boot/grub/grub.conf):

`ro root=/dev/... clocksource=tsc`

You can also change the clock source while the system is running  e.g., to
force use of HPET:

<pre>
$ echo hpet > /sys/devices/system/clocksource/clocksource0/current_clocksource
</pre>

The above discusssion refers to what I will call hardware clocks, although
strictly speaking these clocks are a mixture of hardware and software. At the
bottom of it all there's some kind of hardware device that generates periodic
timing pulses, which are then counted to create the clock. In some cases (e.g.,
the TSC) the counting is done in hardware, while in others (e.g., jiffies) the
counting is done in software.

Wall-Clock Time
---------------

The hardware (or hardware/software hybrid) clocks just discussed all have one
thing in common: they are simply counters, and as such have no direct
relationship to what most of us think of as time, commonly referred to as
wall-clock time.

To derive wall-clock time from these counters requires some fairly intricate
software, at least if the wall-clock time is to be reasonably accurate. What
reasonably accurate means of course depends on how important it is (i.e., how
much money is available) to make sure that wall-clock time is accurate. 

The whole process of synchronizing multiple distributed clocks is hellishly complicated, and we're not going to go into it here. There are many different mechanisms for synchronizing distributed clocks, from the relatively simple (e.g., NTP[^3]) to the not-quite-so-simple (e.g., PTP[^4]), up to specialized proprietary solutions[^5].

The main point is that synchronizing a system's wall-clock time with other
systems requires a way to adjust the clock to keep it in sync with its peers.
There are two ways this can be done:

-   Stepping is the process of making (one or more) discontinuous changes to
    the wall-clock component of the system time. This can cause big jumps in the
    wall-clock time, including backwards jumps, although the time adjustment
    software can often be configured to limit the size of a single change. A
    common example is a system that is configured to initialize its clock at
    boot time from an NTP server.

-   Slewing (sometimes called disciplining) involves actually changing the frequency (or frequency
    multiplier) of the oscillator used to drive a hardware counter like the TSC.
    This can cause the clock to run relatively faster or slower, but it cannot
    jump, and so cannot go backwards.
    


Available Clock Sources
-----------------------

The most common way to get time information in Linux is by calling the
gettimeofday() system call, which returns the current wall-clock time with
microsecond precision (although not necessarily microsecond accuracy). Since
gettimeofday() calls clock_gettime(CLOCK_REALTIME, ), the following discussion
applies to it as well.

Linux also implements the POSIX clock_gettime() family of functions, which let
you query different clock sources, including:

<table id="mytab">
<tbody>
<tr>
  <td>CLOCK_REALTIME </td>
  <td>Represents wall-clock time. Can be both stepped and slewed by time adjustment code (e.g., NTP, PTP).</td>
</tr>
<tr>
  <td>CLOCK_REALTIME_COARSE </td>
  <td>A lower-resolution version of CLOCK_REALTIME.</td>
</tr>
<tr>
  <td>CLOCK_REALTIME_HR  </td>
  <td>A higher-resolution version of CLOCK_REALTIME. 
                        Only available with the real-time kernel.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC </td>
  <td>Represents the interval from an abitrary time. 
                        Can be slewed but not stepped by time adjustment code. 
                        As such, it can only move forward, not backward.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC_COARSE </td>
  <td>A lower-resolution version of CLOCK_MONOTONIC.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC_RAW </td>
  <td>A version of CLOCK_MONOTONIC that can neither be slewed nor stepped by time adjustment code.</td>
</tr>
<tr>
  <td>CLOCK_BOOTTIME</td>
  <td>A version of CLOCK_MONOTONIC that additionally reflects time spent in suspend mode.  Only available in newer (2.6.39+) kernels.</td>
</tr>
</tbody>
</table>
<br>

The availability of the various clocks, as well as their resolution and
accuracy, depends on the hardware as well as the specific Linux implementation.
As part of the [accompanying source code](<https://github.com/btorpey/clocks.git>) for this article I've
included a small test program (clocks.c) that when compiled[^6] and run will
print the relevant information about the clocks on a system. On my test
machine[^7] it shows the following:

<pre>
clocks.c
                    clock	       res (ns)	           secs	          nsecs
             gettimeofday	          1,000	  1,391,886,268	    904,379,000
           CLOCK_REALTIME	              1	  1,391,886,268	    904,393,224
    CLOCK_REALTIME_COARSE	        999,848	  1,391,886,268	    903,142,905
          CLOCK_MONOTONIC	              1	        136,612	    254,536,227
      CLOCK_MONOTONIC_RAW	    870,001,632	        136,612	    381,306,122
   CLOCK_MONOTONIC_COARSE	        999,848	        136,612	    253,271,977
</pre>

Note that it's important to pay attention to what clock_getres() returns -- a particular clock source can (and does, as can be seen above with the COARSE clocks) sometimes return what may look like higher-precision values, but any digits beyond its actual precision are likely to be garbage.  (The exception is gettimeofday -- since it returns a timeval, which is denominated in micros, the lower-order digits are all zeros).

Also, the value returned from clock_getres() for CLOCK_MONOTONIC_RAW is clearly garbage, although I've seen similar results on several machines.

Finally, note that the resolution listed for CLOCK_REALTIME is close to, but not
quite, 1 million -- this is an artifact of the fact that the oscillator cannot
generate a frequency of exactly 1000 Hz -- it's actually 1000.15 Hz.

Getting Clock Values in Software
--------------------------------

Next up is a brief discussion of how to read these different clock values from
software.

<a name="rdtscp"></a>
### Assembler

In assembler language, the RDTSC instruction returns the value of the TSC
directly in registers edx:eax. However, since modern CPU's support out-of-order
execution, it has been common practice to insert a serializing instruction (such
as CPUID) prior to the RDTSC instruction in order to ensure that the execution
of RDTSC is not reordered by the processor.

More recent CPU's include the RDTSCP instruction, which does any necessary
serialization itself. This avoids the overhead of the CPUID instruction, which
can be considerable (and variable). If your CPU supports RDTSCP, use that instead of the
CPUID/RDTSC combination.

### C/C++

Obviously, the RDTSC instruction can be called directly from C or C++, using
whatever mechanism your compiler provides for accessing assembler language, or
by calling an assembler stub that is linked with the C/C++ program. (An example
can be found at [Agner Fog's excellent website](<http://agner.org/optimize/#asmlib>)).

Calling gettimeofday() or clock_gettime() is pretty straightforward -- see the
accompanying [clocks.c source file](<https://github.com/btorpey/clocks/blob/master/clocks.c>) for examples.



### Java

Java has only two methods that are relevant to this discussion:

-   System.currentTimeMillis() returns the current wall-clock time as the number
    of milliseconds since the epoch. It calls gettimeofday(), which in turn
    calls clock_gettime(CLOCK_REALTIME, ...).

-   System.nanoTime returns the number of nanoseconds since some unspecified
    starting point. Depending on the capabilities of the system, it either calls
    gettimeofday(), or clock_gettime(CLOCK_MONOTONIC, ).

The bad news is that if you need clock values other than the above in Java,
you're going to need to roll your own, e.g. by calling into C via JNI. The good
news is that doing so is not much more expensive than calling nanoTime (at least in my tests).

### Overhead of Clock Queries

The Heisenberg Uncertainty Principle says, in a nutshell, that the act of
observing a phenomenom changes it. A similar issue exists with getting
timestamps for latency measurement, since it takes a finite (and sometimes
variable) amount of time to read any clock source.  In other words, just because the TSC on a 2GHz machine ticks twice per nanosecond doesn't mean we can measure intervals of a nanosecond -- we also need to account for the time it takes to read the TSC from software.

So, how expensive is it to perform these different clock queries? Included is some [sample code](<https://github.com/btorpey/clocks.git>) that you can
use to measure the time it takes to query various clock sources, from both C++
and Java (using JNI to call C code).

Both the C++ and Java versions take the same approach: call the particular clock
function in a tight loop, and store the result. We do this a large number of
times, and hang on to the results from the final iteration. This has the effect
of allowing Java to do any jitting it needs to, and for both the C++ and Java
versions to help ensure that code and data is in the processor's cache memory.

The results of running the test on my test machine are (all timings are in nanoseconds):

<pre>

ClockBench.cpp
                   Method       samples     min     max     avg  median   stdev
           CLOCK_REALTIME       255       51.00   57.00   53.60   54.00    1.22
    CLOCK_REALTIME_COARSE       255        0.00    0.00    0.00    0.00    0.00
          CLOCK_MONOTONIC       255       51.00   57.00   53.80   54.00    1.10
      CLOCK_MONOTONIC_RAW       255      653.00 1011.00  691.04  832.00   48.31
   CLOCK_MONOTONIC_COARSE       255        0.00    0.00    0.00    0.00    0.00
              cpuid+rdtsc       255       93.00   94.00   93.23   93.50    0.42
                    rdtsc       255       24.00   28.00   25.18   26.00    1.49
Using CPU frequency = 2.660000

ClockBench.java
                   Method       samples     min     max     avg  median   stdev
          System.nanoTime       255       54.00   58.00   55.31   56.00    1.51
           CLOCK_REALTIME       255        0.00 1000.00   66.67  500.00  249.44
              cpuid+rdtsc       255      108.00  109.00  108.27  108.50    0.44
                    rdtsc       255       39.00   43.00   39.48   41.00    1.04
Using CPU frequency = 2.660000
</pre>


A few things to note about these results:

-   Both of the COARSE clocks show a latency of zero for getting the clock
    value. This tells us that the time it takes to
    get the clock value is less than the resolution of the clock. (Our previous
    test showed a resolution of 1ms for the COARSE clocks).

-   For some reason, the CLOCK_MONOTONIC_RAW clock is very expensive to query. I
    can't explain this --  you would think that its lack of adjustment would make
    it faster, not slower. This is unfortunate, as otherwise it would be an
    excellent choice for intra-machine timing.

-   As you might expect, the combination of cpuid and rdtsc is slower than
    rdtscp, which is slower than rdtsc alone. In general, this would
    suggest that rdtscp should be preferred if available, with a fallback to
    cpuid+rdtsc if not. (While rdtsc alone is the fastest, the fact that it can
    be inaccurate as a result of out-of-order execution means it is only useful
    for timing relatively long operations where that inaccuracy is not
    significant -- but those are precisely the scenarios where its speed is less
    important).

-   Also as expected, the Java versions are slightly slower than the C++
    versions, presumably due to the overhead of going through JNI.
    
### Update
Well, I admit that Java is not my strong suit, but I nevertheless understand the implications
of "warm-up" and JIT-ing when benchmarking Java code.  My understanding (and the docs seem to agree)
is that Java methods get JIT-ed after approx. 10,000 invocations.  I also thought (and still do) that 
100 iterations of 200 invocations would be more than 10,000.  Whatever -- I adjusted the number of iterations
for the Java benchmark, and that made a big difference -- especially in the timings for System.nanotime, which now
agree much more closely with other published benchmarks, specifically the results published in 
["Nanotrusting the Nanotime"](<http://shipilev.net/blog/2014/nanotrusting-nanotime/>).  Thanks, Aleksey!

Conclusion
----------

I thought this would be a very brief and somewhat trivial research project. In
fact, it turned out to be far more complicated (and less well-documented) than I
expected. I guess I should have known: everything related to time and computers
turns out to be a major pain in the neck!

Anyway, I hope this proves helpful. (I know I would have been very happy to have
had this when I started looking into clock sources).  

As always, please feel free to [contact me](<mailto:wallstprog@gmail.com>)
directly with comments, suggestions, corrections, etc.

Additional Resources
--------------------

Following are the main anchor points that I kept coming back to you as I
researched this article.

<http://elinux.org/Kernel_Timer_Systems>

<http://elinux.org/High_Resolution_Timers>

<http://juliusdavies.ca/posix_clocks/clock_realtime_linux_faq.html>

<http://en.wikipedia.org/wiki/Time_Stamp_Counter>

<http://stackoverflow.com/questions/10921210/cpu-tsc-fetch-operation-especially-in-multicore-multi-processor-environment>

<http://www.citihub.com/requesting-timestamp-in-applications/>

<http://www.intel.com/content/www/us/en/intelligent-systems/embedded-systems-training/ia-32-ia-64-benchmark-code-execution-paper.html>



[^1]:  The best case being hardware on each machine with a CSAC (chip-scale atomic clock) or OCXO (oven-controlled crystal oscillator). These can be a bit pricey, however.

[^2]:  The accuracy of a typical RTC in a PC-type computer is rated at +/- 20ppm, so it can gain or lose 20 us each second. This turns out to be approximately one minute per month, which may be OK for a cheap digital watch, but for a computer is not too good. For more information, see <http://www.maximintegrated.com/app-notes/index.mvp/id/58>.

[^3]: Network Time Protocol, RFC 1305 (<https://tools.ietf.org/html/rfc1305>)

[^4]: Precision Time Protocol, IEEE 1588 (<http://www.nist.gov/el/isd/ieee/ieee1588.cfm>)

[^5]: From companies like Symmetricon, Corvil, TS Associates and others.

[^6]:  Note that the program must be compiled, as well as run, on the target system -- it uses the presence or absence of pre-processor symbols to determine whether a particular clock source is available.

[^7]: CentOS 6.5 running on a Dell 490 with dual Xeon 5150's at 2.6 GHz.

</p>
]]></content>
  </entry>
  
</feed>
